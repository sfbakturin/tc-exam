# Декодирование с мягким выходом. Алгоритм БКЕР

Построение хороших кодов и декодеров для них - дело сложное. Очень часто такую задачу решают с помощью *комбинирования длинных кодов*. Декодеры таких кодов строится из декодеров *компонентных кодов*, которые некоторым образом обмениваются информацией о *надёжности* отдельных символов кодовых слов.

Под **надёжностью** подразумевается следующее:

$$
  p = \{c_{i} = a \ | \ y_{0}^{n - 1}\} = \sum_{c \in C_{i}(a)}{p\{c \ | \ y_{0}^{n - 1}\}},
$$

то есть это вероятность того, что $i$-ый символ кодового слова был равен $a$, при условии наблюдения всего принятого вектора $y$, где

* $y_{0}^{n - 1} = (y_{0}, \ldots, y_{n - 1})$ - результат передачи кодового слова кода $C$ по каналу без памяти
* $C_i(a) = \left\{\left(c_0, \ldots, c_{n - 1}\right) \in C \ | \ c_i = a\right\}$.

Иногда это удобно представить в виде апостериорных логарифмических отношений правдоподобия для двоичных кодов:

$$
  L_{i} = \ln{\left(\dfrac{p\left\{c_{i} = 0\ | \ y_{0}^{n - 1}\right\}}{p\left\{c_{i} = 1\ | \ y_{0}^{n - 1}\right\}}\right)}
$$

Здесь появляется задача декодирования вектора, имея в качестве исходных данных логарифмические отношения правдоподобия для отдельных символов кодового слова $L_{i} = \ln{\left(\dfrac{p(y_i \ | \ c_i = 0)}{p(y_i \ | \ c_i = 1)}\right)}$ - здесь мы считаем логарифмическое правдоподобие только глядя на один единственный символ $y_i$, а вычислить условную вероятность (или условное логарифмическое правдоподобие) при условии наблюдения всех принятых символов.

---

Оказывается, эту задачу можно решить с помощью *алгоритма Бала-Коке-Елинека-Равива*. Данный алгоритм требует построения решетки и использует следующие обозначения:

* $s'$, $\hat{s}$ -  состояния (узлы) решетки
* $\delta(s', s)$ - метка на ребре между узлами $s'$ и $s$ - равно 0 или 1
* $V_i$ - множество узлов решетки на ярусе $i$
* $S_{\delta}^{i}$ - множество пар состояний $s' \in V_i, ~ s \in V_{i + 1}$, ребро между которых имеет метку $\delta$

Алгоритм вытекает из следующего соотношения:

$$
  L_{i} = \ln{\left(\dfrac{P\{c_{i} = 0 \ | \ y_{0}^{n - 1}\}}{P\{c_{i} = 1\ |\ y_{0}^{n - 1}\}}\right)} = \ln{\left(\dfrac{\displaystyle\sum_{(s', s) \in S_0}{\left(\dfrac{p\left(s_i = s', ~ s_{i + 1} = s, ~ y_{0}^{n - 1}\right)}{p_{0}^{n - 1}}\right)}}{\displaystyle\sum_{(s', s) \in S_{1}}{\left(\dfrac{p\left(s_i = s', ~ s_{i + 1} = s, ~ y_0^{n - 1}\right)}{p\left(y_0^{n - 1}\right)}\right)}}\right)}
$$

Объяснение формулы:

1. Логарифмическое отношение правдоподобия $L_i$:
   * $L_i$​ измеряет, насколько вероятно, что бит $c_i$ равен 0 по сравнению с тем, что он равен 1, с учетом всех наблюдаемых данных $y_0^{n - 1}$.
   * Если $L_i > 0$, то бит $c_i$ скорее всего равен 0, если $L_i < 0$, то 1.
2. Вероятности в числителе и знаменателе:
   * Числитель представляет собой суммарную вероятность для всех переходов между состояниями $(s', s)$, которые соответствуют $c_i = 0$.
   * Числитель представляет собой суммарную вероятность для всех переходов между состояниями $(s', s)$, которые соответствуют $c_i = 1$.
3. Условные вероятности:
   * Вероятности переходов и наблюдений $p\left(s_i = s', ~ s_{i + 1} = s, ~ y_{0}^{n - 1}\right)$ вычисляются на основе скрытой марковской модели, которая описывает динамику состояний и эмиссии наблюдений.
4. Нормализация по $p\left(y_{0}^{n - 1}\right)$:
   * В обоих случаях вероятность нормализуется на $p\left(y_{0}^{n - 1}\right)$, чтобы получить условные вероятности наблюдений при заданных состояниях.

Алгоритм:

1. Нахождение логарифмических отношений правдоподобия отдельных символов кодового слова $L(c_i)$, для $i \in 0 \ldots n - 1$. Что здесь происходит?
   1. Логарифмическое отношение правдоподобия $L(c_i)$:
      * Это мера уверенности в том, что конкретный символ $c_i$ в кодовом слове является 0 или 1.
      * Логарифмическое отношение правдоподобия определяется как $L(c_i) = \log{\left(\dfrac{P(c_i = 0\ |\ \text{полученные данные})}{P(c_i = 1\ |\ \text{полученные данные})}\right)}$, где $P(...)$ — вероятности того, что $c_i$ равен 0 или 1, учитывая полученные данные (например, закодированное сообщение).
   2. Зачем это нужно?
      * Вместо того чтобы просто говорить, что символ больше похож на 0 или 1, используется логарифмическое отношение, чтобы учитывать степень уверенности.
      * Если $L(c_i)$ положительное, это говорит о том, что символ, скорее всего, равен 0. Если отрицательное — 1.

2. Вычисление величин $\gamma_{k}(s', s) = P\{c_i = \delta(s', s)\}p(y_i\ |\ c_i = \delta(s', s))$, где вероятность $P\{c_i = \delta(s', s)\}$ представляет собой априорную вероятность того, что этот бит равен метке $\delta(s', s)$ перехода между состояниями $s'$, $s$.
   1. Что здесь происходит?
      1. Величина $\gamma_{k}(s', s)$:
         * Это вероятность того, что в момент $k$ произошел переход из состояния $s'$ в состояние $s$, и при этом был передан определенный бит $c_i$​, связанный с этим переходом.
      2. Разберем формулу:
         * $P\{c_i = \delta(s', s)\}$: Априорная вероятность того, что бит $c_i$ соответствует значению $\delta(s', s)$, определенному переходом между состояниями $s'$ и $s$.
         * $p(y_i\ |\ c_i = \delta(s' ,s))$: Условная вероятность того, что наблюдаемое значение $y_i$ соответствует переданному биту $c_i$.
      3. Объяснение компонент:
         * $\delta(s', s)$: Это функция, которая определяет, какой бит передается при переходе из состояния $s'$ в $s$. Например, может быть 0 или 1.
         * Априорная вероятность $P\{c_i = \delta(s', s)\}$: Это просто предположение о том, как часто бит $c_i$ равен 0 или 1 для данного перехода.
         * Вероятность $p(y_i\ |\ c_i)$: Это вероятность того, что мы наблюдаем значение $y_i$ при переданном бите $c_i$. Обычно это вероятностная модель канала (например, с шумом).
   2. Зачем это нужно?
      1. Это нужно для вычисления вероятности того, что определенный переход между состояниями соответствует переданному биту, основываясь на том, что мы наблюдаем (значение $y_i$).
      2. Этот шаг помогает определить, как вероятны определенные переходы в модели, что важно для алгоритмов декодирования, таких как алгоритм Витерби.

3. Вычисление $\alpha'_{i}(s)$ (прямая рекурсия). Это вероятность того, что система находится в состоянии $s$ в момент времени $i$, при условии, что была получена последовательность наблюдений $y_1, ~ y_2, ~ \ldots, ~ y_i$ до этого момента. Считается рекурсия фо формуле: $\alpha'_{i}(s) = \dfrac{\displaystyle\sum_{\hat{s} \in V_{i - 1}}{\left(\alpha_{i - 1}(\hat{s}) \cdot \gamma_{i}(\hat{s}, s)\right)}}{\displaystyle\sum_{s' \in V_i}{\left(\displaystyle\sum_{\hat{s} \in V_{i - 1}}{\left(\alpha_{i - 1}(\hat{s}) \cdot \gamma_{i}(\hat{s}, s')\right)}\right)}}$, $0 < i \leqslant n$.
   * Прямой ход (или рекурсия) — это способ пройтись по всем возможным состояниям системы шаг за шагом и рассчитать, насколько вероятно, что система находится в каждом из них в данный момент, учитывая наблюдаемые данные.
   * Мы как бы собираем информацию от начала наблюдений и "строим" вероятность того, что система попала в определенное состояние, учитывая все наблюдения до текущего момента.
4. Вычисление $\beta'_{i}(\hat{s})$ (обратная рекурсия). Это вероятность наблюдения оставшейся последовательности $y_{i + 1}, ~ y_{i + 2}, ~ \ldots, y_T$​, при условии, что система находилась в состоянии $s$ в момент времени $i$. Считается рекурсия фо формуле: $\beta'_{i}(\hat{s}) = \dfrac{\displaystyle\sum_{s \in V_{i + 1}}{\left(\gamma_{i + 1}(\hat{s}, s) \cdot \beta'_{i + 1}(s)\right)}}{\displaystyle\sum_{s \in V_{i}}{\left(\displaystyle\sum_{s' \in V_{i + 1}}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s)\right)}\right)}}$, $0 \leqslant i < n$.
   * Обратная рекурсия или обратный ход — это процесс вычисления вероятности будущих наблюдений, начиная с конца последовательности и двигаясь назад к началу.
   * Она помогает понять, насколько вероятны наблюдаемые данные после текущего состояния, если известно текущее состояние системы.
5. Вычисление апостериорных логарифмических отношений правдоподобия $L_i$, $0 \leqslant i < n$, информационных битов согласно $L_i = \ln{\dfrac{\displaystyle\sum_{(s', s) \in S_1}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s) \cdot \beta'_{i + 1}(s)\right)}}{\displaystyle\sum_{(s' ,s) \in S_0}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s) \cdot \beta'_{i + 1}(s)\right)}}}$.
   * Здесь:
     * $\alpha'_i(s')$: Вероятность того, что система находится в состоянии $s'$ в момент времени $i$, учитывая все предыдущие наблюдения.
     * $\gamma_{i + 1}(s', s)$: Вероятность перехода из состояния $s'$ в состояние $s$ на следующем шаге времени, учитывая, что при этом передается определенный бит, определяется функцией $\delta(s', s)$.
     * $\beta'_{i + 1}(s)$: Вероятность того, что оставшиеся наблюдения будут такими, как мы их видели, если система находится в состоянии $s$ в момент времени $i + 1$.
     * Множества $S_1$ и $S_0$: множество пар состояний $(s', s)$, для которых передаваемый бит $\delta(s', s) = 1/0$.
   * Смысл формулы:
     * Апостериорное логарифмическое отношение правдоподобия $L_i$: это логарифм отношения вероятностей того, что информационный бит $i$ равен 1 к вероятности того, что он равен 0, с учетом всех наблюдений.
     * Это отношение помогает определить, как вероятно, что конкретный бит равен 1 или 0, с учетом всей информации о переходах и наблюдениях.
     * Значение $L_i$ используется для принятия решения о значении бита: если $L_i > 0$, то $i$-й бит, скорее всего, равен 1, если $L_i < 0$, то он равен 0.
6. Принятие решения относительно каждого символа. Этот заключительный шаг включает принятие решения о значении каждого символа (или бита) на основе апостериорных логарифмических отношений правдоподобия $L_i$, которые были вычислены на предыдущем этапе, если положительный, то бит $c_i$ принимается равным 1, иначе - 0.

**Примечание**. Полученная последовательность решений может не являться кодовым словом.
