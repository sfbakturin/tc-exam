# Декодирование с мягким выходом. Алгоритм БКЕР

Построение хороших кодов и декодеров для них - дело сложное. Очень часто такую задачу решают с помощью *комбинирования длинных кодов*. Декодеры таких кодов строится из декодеров *компонентных кодов*, которые некоторым образом обмениваются информацией о *надёжности* отдельных символов кодовых слов.

Под **надёжностью** подразумевается следующее:

$$
  p  \{c_{i} = a \ | \ y_{0}^{n - 1}\} = \sum_{c \in C_{i}(a)}{p\{c \ | \ y_{0}^{n - 1}\}},
$$

то есть это вероятность того, что $i$-ый символ кодового слова был равен $a$, при условии наблюдения всего принятого вектора $y$, где

* $y_{0}^{n - 1} = (y_{0}, \ldots, y_{n - 1})$ - результат передачи кодового слова кода $C$ по каналу без памяти
* $C_i(a) = \left\{\left(c_0, \ldots, c_{n - 1}\right) \in C \ | \ c_i = a\right\}$.

Иногда это удобно представить в виде апостериорных логарифмических отношений правдоподобия для двоичных кодов:

$$
  L_{i} = \ln{\left(\dfrac{p\left\{c_{i} = 0\ | \ y_{0}^{n - 1}\right\}}{p\left\{c_{i} = 1\ | \ y_{0}^{n - 1}\right\}}\right)}
$$

Здесь появляется задача декодирования вектора, имея в качестве исходных данных логарифмические отношения правдоподобия для отдельных символов кодового слова $L_{i} = \ln{\left(\dfrac{p(y_i \ | \ c_i = 0)}{p(y_i \ | \ c_i = 1)}\right)}$ - здесь мы считаем логарифмическое правдоподобие только глядя на один единственный символ $y_i$, а вычислить условную вероятность (или условное логарифмическое правдоподобие) при условии наблюдения всех принятых символов.

---

Оказывается, эту задачу можно решить с помощью *алгоритма Бала-Коке-Елинека-Равива*. Данный алгоритм требует построения решетки и использует следующие обозначения:

* $s'$, $\hat{s}$ -  состояния (узлы) решетки
* $\delta(s', s)$ - метка на ребре между узлами $s'$ и $s$ - равно 0 или 1
* $V_i$ - множество узлов решетки на ярусе $i$
* $S_{\delta}^{i}$ - множество пар состояний $s' \in V_i, ~ s \in V_{i + 1}$, ребро между которых имеет метку $\delta$

Алгоритм вытекает из следующего соотношения:

$$
  L_{i} = \ln{\left(\dfrac{P\{c_{i} = 0 \ | \ y_{0}^{n - 1}\}}{P\{c_{i} = 1\ |\ y_{0}^{n - 1}\}}\right)} = \ln{\left(\dfrac{\displaystyle\sum_{(s', s) \in S_0}{\left(\dfrac{p\left(s_i = s', ~ s_{i + 1} = s, ~ y_{0}^{n - 1}\right)}{p\left(y_0^{n - 1}\right)}\right)}}{\displaystyle\sum_{(s', s) \in S_{1}}{\left(\dfrac{p\left(s_i = s', ~ s_{i + 1} = s, ~ y_0^{n - 1}\right)}{p\left(y_0^{n - 1}\right)}\right)}}\right)}
$$

Объяснение формулы:

1. Логарифмическое отношение правдоподобия $L_i$:
   * $L_i$​ измеряет, насколько вероятно, что бит $c_i$ равен 0 по сравнению с тем, что он равен 1, с учетом всех наблюдаемых данных $y_0^{n - 1}$.
   * Если $L_i > 0$, то бит $c_i$ скорее всего равен 0, если $L_i < 0$, то 1.
2. Вероятности в числителе и знаменателе:
   * Числитель представляет собой суммарную вероятность для всех переходов между состояниями $(s', s)$, которые соответствуют $c_i = 0$.
   * Числитель представляет собой суммарную вероятность для всех переходов между состояниями $(s', s)$, которые соответствуют $c_i = 1$.
3. Условные вероятности:
   * Вероятности переходов и наблюдений $p\left(s_i = s', ~ s_{i + 1} = s, ~ y_{0}^{n - 1}\right)$ вычисляются на основе скрытой марковской модели, которая описывает динамику состояний и эмиссии наблюдений.
4. Нормализация по $p\left(y_{0}^{n - 1}\right)$:
   * В обоих случаях вероятность нормализуется на $p\left(y_{0}^{n - 1}\right)$, чтобы получить условные вероятности наблюдений при заданных состояниях.

Алгоритм:

1. Нахождение логарифмических отношений правдоподобия отдельных символов кодового слова $L(c_i)$, для $i \in 0 \ldots n - 1$. Что здесь происходит?
   1. Логарифмическое отношение правдоподобия $L(c_i)$:
      * Это мера уверенности в том, что конкретный символ $c_i$ в кодовом слове является 0 или 1.
      * Логарифмическое отношение правдоподобия определяется как $L(c_i) = \log{\left(\dfrac{P(c_i = 0\ |\ \text{полученные данные})}{P(c_i = 1\ |\ \text{полученные данные})}\right)}$, где $P(...)$ — вероятности того, что $c_i$ равен 0 или 1, учитывая полученные данные (например, закодированное сообщение).
   2. Зачем это нужно?
      * Вместо того чтобы просто говорить, что символ больше похож на 0 или 1, используется логарифмическое отношение, чтобы учитывать степень уверенности.
      * Если $L(c_i)$ положительное, это говорит о том, что символ, скорее всего, равен 0. Если отрицательное — 1.

2. Вычисление величин $\gamma_{k}(s', s) = P\{c_i = \delta(s', s)\}p(y_i\ |\ c_i = \delta(s', s))$, где вероятность $P\{c_i = \delta(s', s)\}$ представляет собой априорную вероятность того, что этот бит равен метке $\delta(s', s)$ перехода между состояниями $s'$, $s$.
   1. Что здесь происходит?
      1. Величина $\gamma_{k}(s', s)$:
         * Это вероятность того, что в момент $k$ произошел переход из состояния $s'$ в состояние $s$, и при этом был передан определенный бит $c_i$​, связанный с этим переходом.
      2. Разберем формулу:
         * $P\{c_i = \delta(s', s)\}$: Априорная вероятность того, что бит $c_i$ соответствует значению $\delta(s', s)$, определенному переходом между состояниями $s'$ и $s$.
         * $p(y_i\ |\ c_i = \delta(s' ,s))$: Условная вероятность того, что наблюдаемое значение $y_i$ соответствует переданному биту $c_i$.
      3. Объяснение компонент:
         * $\delta(s', s)$: Это функция, которая определяет, какой бит передается при переходе из состояния $s'$ в $s$. Например, может быть 0 или 1.
         * Априорная вероятность $P\{c_i = \delta(s', s)\}$: Это просто предположение о том, как часто бит $c_i$ равен 0 или 1 для данного перехода.
         * Вероятность $p(y_i\ |\ c_i)$: Это вероятность того, что мы наблюдаем значение $y_i$ при переданном бите $c_i$. Обычно это вероятностная модель канала (например, с шумом).
   2. Зачем это нужно?
      1. Это нужно для вычисления вероятности того, что определенный переход между состояниями соответствует переданному биту, основываясь на том, что мы наблюдаем (значение $y_i$).
      2. Этот шаг помогает определить, как вероятны определенные переходы в модели, что важно для алгоритмов декодирования, таких как алгоритм Витерби.

3. Вычисление $\alpha'_{i}(s)$ (прямая рекурсия). Это вероятность того, что система находится в состоянии $s$ в момент времени $i$, при условии, что была получена последовательность наблюдений $y_1, ~ y_2, ~ \ldots, ~ y_i$ до этого момента. Считается рекурсия фо формуле: $\alpha'_{i}(s) = \dfrac{\displaystyle\sum_{\hat{s} \in V_{i - 1}}{\left(\alpha_{i - 1}(\hat{s}) \cdot \gamma_{i}(\hat{s}, s)\right)}}{\displaystyle\sum_{s' \in V_i}{\left(\displaystyle\sum_{\hat{s} \in V_{i - 1}}{\left(\alpha_{i - 1}(\hat{s}) \cdot \gamma_{i}(\hat{s}, s')\right)}\right)}}$, $0 < i \leqslant n$.
   * Прямой ход (или рекурсия) — это способ пройтись по всем возможным состояниям системы шаг за шагом и рассчитать, насколько вероятно, что система находится в каждом из них в данный момент, учитывая наблюдаемые данные.
   * Мы как бы собираем информацию от начала наблюдений и "строим" вероятность того, что система попала в определенное состояние, учитывая все наблюдения до текущего момента.
4. Вычисление $\beta'_{i}(\hat{s})$ (обратная рекурсия). Это вероятность наблюдения оставшейся последовательности $y_{i + 1}, ~ y_{i + 2}, ~ \ldots, y_n$​, при условии, что система находилась в состоянии $s$ в момент времени $i$. Считается рекурсия фо формуле: $\beta'_{i}(\hat{s}) = \dfrac{\displaystyle\sum_{s \in V_{i + 1}}{\left(\gamma_{i + 1}(\hat{s}, s) \cdot \beta'_{i + 1}(s)\right)}}{\displaystyle\sum_{s \in V_{i}}{\left(\displaystyle\sum_{s' \in V_{i + 1}}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s)\right)}\right)}}$, $0 \leqslant i < n$.
   * Обратная рекурсия или обратный ход — это процесс вычисления вероятности будущих наблюдений, начиная с конца последовательности и двигаясь назад к началу.
   * Она помогает понять, насколько вероятны наблюдаемые данные после текущего состояния, если известно текущее состояние системы.
5. Вычисление апостериорных логарифмических отношений правдоподобия $L_i$, $0 \leqslant i < n$, информационных битов согласно $L_i = \ln{\dfrac{\displaystyle\sum_{(s', s) \in S_1}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s) \cdot \beta'_{i + 1}(s)\right)}}{\displaystyle\sum_{(s' ,s) \in S_0}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s) \cdot \beta'_{i + 1}(s)\right)}}}$.
   * Здесь:
     * $\alpha'_i(s')$: Вероятность того, что система находится в состоянии $s'$ в момент времени $i$, учитывая все предыдущие наблюдения.
     * $\gamma_{i + 1}(s', s)$: Вероятность перехода из состояния $s'$ в состояние $s$ на следующем шаге времени, учитывая, что при этом передается определенный бит, определяется функцией $\delta(s', s)$.
     * $\beta'_{i + 1}(s)$: Вероятность того, что оставшиеся наблюдения будут такими, как мы их видели, если система находится в состоянии $s$ в момент времени $i + 1$.
     * Множества $S_1$ и $S_0$: множество пар состояний $(s', s)$, для которых передаваемый бит $\delta(s', s) = 1/0$.
   * Смысл формулы:
     * Апостериорное логарифмическое отношение правдоподобия $L_i$: это логарифм отношения вероятностей того, что информационный бит $i$ равен 1 к вероятности того, что он равен 0, с учетом всех наблюдений.
     * Это отношение помогает определить, как вероятно, что конкретный бит равен 1 или 0, с учетом всей информации о переходах и наблюдениях.
     * Значение $L_i$ используется для принятия решения о значении бита: если $L_i > 0$, то $i$-й бит, скорее всего, равен 1, если $L_i < 0$, то он равен 0.
6. Принятие решения относительно каждого символа. Этот заключительный шаг включает принятие решения о значении каждого символа (или бита) на основе апостериорных логарифмических отношений правдоподобия $L_i$, которые были вычислены на предыдущем этапе, если положительный, то бит $c_i$ принимается равным 1, иначе - 0.

**Примечание**. Полученная последовательность решений может не являться кодовым словом.

---

### Пацанский набор от @sn1tr0n:

#### Чё это? 

Алгоритм БКЕР позволяет найти апостериорные (то есть $c_i | y$) логарифмические отношения правдоподобия $L_i$ для каждого символа слова. Надо это для работы сложных декодеров, которые могут принимать в расчёт при работе как раз эту меру *надёжности* декодирования $i$-го символа. Причём важно, что надёжность эта измеряется относительно **всего** пришедшего вектора $y$, а не только соответствующего символа.

Алгоритм использует для своей работы решётку, которую мы строили в предыдущих билетах.
>  ℹ️ от Соника: придумывался алгос для непрерывных решёток, но для конечных (которые в просто линейных и (?) свёрточных кодах) его реализовать гораздо проще, и используют на практике его.

#### Как это?

Важное основание всего алгоритма:

$$
  L_{i} = \ln{\left(\dfrac{P\{c_{i} = 0 \ | \ y_{0}^{n - 1}\}}{P\{c_{i} = 1\ |\ y_{0}^{n - 1}\}}\right)} = \ln{\left(\dfrac{\displaystyle\sum_{(s', s) \in S_0}{\left(\dfrac{p\left(s_i = s', ~ s_{i + 1} = s, ~ y_{0}^{n - 1}\right)}{p\left(y_0^{n - 1}\right)}\right)}}{\displaystyle\sum_{(s', s) \in S_{1}}{\left(\dfrac{p\left(s_i = s', ~ s_{i + 1} = s, ~ y_0^{n - 1}\right)}{p\left(y_0^{n - 1}\right)}\right)}}\right)}
$$

$S_0, S_1$ --- множества рёбер из решётки с символами $0, 1$. $p(y_0^{n - 1})$ --- совместная плотность вероятности всего выхода из канала, $p\left(s_i = s', ~ s_{i + 1} = s, ~ y_{0}^{n - 1}\right)$ --- совместная плотность вероятности состояния кодера и принятых сигналов на шагах $i, i + 1$.

Далее все выкладки повествуют о том, как это считать. Из важного, стоит понять, что числитель можно разложить (важно помнить, что канал не имеет памяти и поведение декодера на символе $i$ зависит только от предыдущего состояния $s'$):

$$p\left(s_i = s', ~ s_{i + 1} = s, ~ y_{0}^{n - 1}\right) = \underbrace{p(s_i = s', y_0^i)}_{\alpha_i(s')} \cdot \underbrace{p(s_{i + 1} = s, y_i | s_i = s')}_{\gamma_{i+1}(s'. s)} \cdot \underbrace{p(y_{i + 1}^{n - 1} |s_{i + 1} = s)}_{\beta_{i+1}(s)}$$

* $\alpha_i(s)$  --- вероятность того, что мы оказались в состоянии $s'$ прочитав первые $i - 1$ бит сообщения (всмотритесь в формулу)
* $\gamma_{i + 1}(s', s)$ --- вероятность того что при обработке $i$-го символа мы перешли из состояния $s'$ в $s$ (и оказались в символе $i + 1$)
* $\beta_{i + 1}(s)$ --- верятность того, что перейдя (оказавшись) в состояние $s$ (на $i$-м шаге) мы будем наблюдать в оставшейся последовательности некоторый $y_{i + 1}^{n - 1}$ (типа, насколько вообще в то, что надо, мы можем попасть, перейдя в состояние $s$)

Ну и всё, далее, супер-теорвером как-то выводятся быстрые формулы для подсчёта этого (не надо пугаться новых "штрихастых" формул --- это отнормированные (?) аналоги, чтобы не терять в точности):

*  $\alpha'_{i}(s) = \dfrac{\displaystyle\sum_{\hat{s} \in V_{i - 1}}{\left(\alpha_{i - 1}(\hat{s}) \cdot \gamma_{i}(\hat{s}, s)\right)}}{\displaystyle\sum_{s' \in V_i}{\left(\displaystyle\sum_{\hat{s} \in V_{i - 1}}{\left(\alpha_{i - 1}(\hat{s}) \cdot \gamma_{i}(\hat{s}, s')\right)}\right)}}$, $0 < i \leqslant n$. --- прямой рекурсией с начальными значениями $\alpha_0'(s) = \alpha_0(s) = 1_{\{s = 0\}}$

* $\gamma_{k}(s', s) = P\{c_i = \delta(s', s)\}p(y_i\ |\ c_i = \delta(s', s))$, где вероятность $P\{c_i = \delta(s', s)\}$ представляет собой априорную вероятность того, что этот бит равен метке $\delta(s', s)$ перехода между состояниями $s'$, $s$.

* $\beta'_{i}(\hat{s}) = \dfrac{\displaystyle\sum_{s \in V_{i + 1}}{\left(\gamma_{i + 1}(\hat{s}, s) \cdot \beta'_{i + 1}(s)\right)}}{\displaystyle\sum_{s \in V_{i}}{\left(\displaystyle\sum_{s' \in V_{i + 1}}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s)\right)}\right)}}$, $0 \leqslant i < n$ --- обратной рекурсий с начальными данными:  $\beta_n'(s) = \beta_n(s) = 1_{\{s = 0\}}$

Алгоритм:

*Все вычисления проводятся для $i \in [0, n)$ (для п. 3 и 4 учитывая начальные значения)*

1. Посчитать априорные $L(c_i) = \ln \frac{p(y_i|c_i = 0)}{p(y_i| c_i=1)}$
2. Посчитать для всех пар $(s', s) \in V_i: ~ \gamma_{i + 1}(s', s)$
   
   Этот алгос может использоваться как составная часть сложного декодера, и на вход ему могут подаваться уже вычисленные апостериорные $L_i^{(e)}$ ЛОПП. Если такие есть, то справедлива формула:

   $$P\{c_i = a\} = \frac{\exp \left(\frac{L_i^{(e)}}{2}\right)}{1 + \exp\left(L_i^{(e)}\right)} \cdot \exp\left((2 a - 1) \frac{L_i^{(e)}}{2}\right)$$

   Если нет, то считаем всё равновероятным $\left(L_i^{(e)} = 0\right)$

3. Считаем $\alpha'_i$ ($i$ включая $n$)
4. Считаем $\beta'_i$
5. Считаем наши (уже апостериорные, не те же, что в п.1) $L_i$:

$$L_i = \ln{\dfrac{\displaystyle\sum_{(s', s) \in S_1}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s) \cdot \beta'_{i + 1}(s)\right)}}{\displaystyle\sum_{(s' ,s) \in S_0}{\left(\alpha'_{i}(s') \cdot \gamma_{i + 1}(s', s) \cdot \beta'_{i + 1}(s)\right)}}}$$

6. Либо передаём результат дальше (например, в другой декодер), либо считаем "кодовый" вектор (как-нибудь жёстко, типа $\hat{c}_i = 1_{L_i > 0}$; обратите внимание, что тут ЛОПП как-бы перевёрнута, положительный результат равен 1; надёжность принимаемого решения также равна $|L_i|$)

**ACHTUNG:** то, что получилосm, может вообще не быть кодовым словом =)
