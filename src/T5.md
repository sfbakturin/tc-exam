# Декодирование по информационным совокупностям

**Информационной совокупностью** называется множество из $k$ позиций в кодовом слове, значения которых однозначно определяют значения символов на остальных позициях кодового слова.

У нас есть какое-то кодовое слово длины $n$, мы каким-то образом выбрали $k$ позиций в нем и сразу угадали значения всех остальных символов.

Если дана какая-то информационная совокупность $\gamma = \{j_1...j_k\}$, то все прочие позиции $\{1....n\} \setminus \gamma$ образуют **проверочную** совокупность (просто все прочие позиции).

Утверждается, что если какие-то позиции $\gamma = {j_1....j_k}$ образуют информационную совокупность, то матрица, составленная из столбцов $j_1....j_k$ порождающей матрицы, обратима.

В каждом кодовом слове позиции, одонозначно определяющие значения символов одни и те же. Информационная совокупность - свойство кода.

Пусть $G(\gamma) = M(\gamma)G$ - порождающая матарица, содержащая единичную подматрицу на столбцах $\gamma$, где $M(\gamma)$ - подходящая обратимая матрица.

ИС свободна от ошибок, если соответствующие позиции вектора $e$ равны 0: $e(\gamma) = 0$.

Декодирование $y = xG + e$ по информационным совокупностям:

- (первоначальный кандидат) $c = 0$;
- Выбрать ИС $\gamma$. Вычислить $c' = y(\gamma)G(\gamma)$;
- Если $d(c', y) < d(c, y)$, то $c = c'$;
- Перейти к следующей ИС. Если все ИС проверены, вернуть $c$;
- Не всякие $k$ позиций образуют информационную совокупность.

Перебирать все информационные совокупноксти необязательно. Если мы на каком-то шаге мы нашли кодовое слово, расстояние которого меньше, чем ($d/2$), то лучшего кодового слова мы не найдем, на этом можно остановиться.

Далеко не любые k позиций образуют ИС.

Для коротких кодов можно заранее построить список ИС, достаточный для исправления всех конфигураций ошибок.

Для длинных кодов такое в общем случае сделать невозможно, потому что их слишком много. Целисообраазно перебирать ИС добавляя и удаляя на каждом шаге по одному элементу. Можно сэкономить на приведении матрицы к каноническому виду (где единичная матрица в начале).

Для длинных кодов целесообразно перебирать ИС, добавляя и удаляя на каждом шаге по одному элементу (экономия на приведении порождающей матрицы к каноническому виду $G(\gamma)$).

Утверждается, что декодирование по информационным совокупностям обеспечивает полное декодирование по минимальному расстоянию.

Сложность декодирования для $(n, k)_q$ кода будет равна $\displaystyle O(L(n)) \, n \, 2^{n \left(b - \left(1 - \frac{n}{k}\right) h\left(\frac{a}{1 - \frac{n}{k}}\right)\right) \left(1 + o(1)\right)}$, где L(n) - сложность приведения матрицы к каноническому виду.

## Декодирование по ИС с исправлением не более t ошибок

Введем понятие покрытия:

**$M(n, m, t)$ покрытием** называется такой набор $F \sub 2^{N_n}$ из подмножеств мощности $m$ множества $N_n = \{1, 2 ... n\}$, что всякое t-элементное подмножество $N_n$ содержиться в одном из $f \in F$.

С помощью покрытия мы можем декодировать по ИС с исправлением уже не более t ошибок. Для этого нам необходимо покрыть все исправимые конфигураации ошибок. Элементы покрытия задают проверочные совокупности.

---
**Пример декодирования** $(7, 4, 3)$ кода, порождаемого $G =$  
$$
\begin{pmatrix}
1 & 1 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 1 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & 1 & 0 \\
1 & 1 & 0 & 0 & 0 & 1 & 1
\end{pmatrix}
$$

- $y = (0, 1, 1, 0, 1, 0, 0)$

- Все возможные конфигурации единичных ошибок покрываются проверочными совокупностями:  
  $M(7, 3, 1) = \{\{1, 2, 4\}, \{5, 6, 7\}, \{3, 4, 5\}\}$

- Им соответствуют информационные совокупности:  
  $\{\{3, 5, 6, 7\}, \{1, 2, 3, 4\}, \{1, 2, 6, 7\}\}$

- Преобразованные порождающие матрицы:
  
$$
\begin{pmatrix}
1 & 1 & \textcolor{red}{1} & 0 & \textcolor{red}{0} & \textcolor{red}{0} & \textcolor{red}{0} \\
1 & 0 & \textcolor{red}{0} & 1 & \textcolor{red}{1} & \textcolor{red}{0} & \textcolor{red}{0} \\
0 & 1 & \textcolor{red}{0} & 1 & \textcolor{red}{0} & \textcolor{red}{1} & \textcolor{red}{0} \\
1 & 1 & \textcolor{red}{0} & 1 & \textcolor{red}{0} & \textcolor{red}{0} & \textcolor{red}{1}
\end{pmatrix}
$$
$$
\begin{pmatrix}
\textcolor{red}{1} & \textcolor{red}{0} & \textcolor{red}{0} & \textcolor{red}{0} & 0 & 1 & 1 \\
\textcolor{red}{0} & \textcolor{red}{1} & \textcolor{red}{0} & \textcolor{red}{0} & 1 & 0 & 1 \\
\textcolor{red}{0} & \textcolor{red}{0} & \textcolor{red}{1} & \textcolor{red}{0} & 1 & 1 & 0 \\
\textcolor{red}{0} & \textcolor{red}{0} & \textcolor{red}{0} & \textcolor{red}{1} & 1 & 1 & 1
\end{pmatrix}
$$
$$
\begin{pmatrix}
\textcolor{red}{1} & \textcolor{red}{0} & 0 & 1 & 1 & \textcolor{red}{0} & \textcolor{red}{0} \\
\textcolor{red}{0} & \textcolor{red}{1} & 1 & 1 & 1 & \textcolor{red}{0} & \textcolor{red}{0} \\
\textcolor{red}{0} & \textcolor{red}{0} & 1 & 0 & 1 & \textcolor{red}{1} & \textcolor{red}{0} \\
\textcolor{red}{0} & \textcolor{red}{0} & 1 & 1 & 0 & \textcolor{red}{0} & \textcolor{red}{1}
\end{pmatrix}
$$

- Кодовые слова, соответствующие под векторам принятого вектора:
  - $(0, 1, \mathbf{1}, 1, 1, 0, 0)$
  - $(0, 1, 1, 0, 0, 1, 1)$
  - $(0, 1, 1, 1, 1, 0, 0)$

- **Построение минимального покрытия** — NP-полная задача.
- **Итеративное построение покрытия** (приближённый алгоритм):
  - На каждом шаге выбирается множество, содержащее максимальное количество непокрытых элементов.
