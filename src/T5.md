# Декодирование по информационным совокупностям

**Информационной совокупностью** называется множество из $k$ позиций в кодовом слове, значения которых однозначно определяют значения символов на остальных позициях кодового слова.

У нас есть какое-то кодовое слово длины $n$, мы каким-то образом выбрали $k$ позиций в нем и сразу угадали значения всех остальных символов.

Если дана какая-то информационная совокупность $\gamma = \{j_1...j_k\}$, то все прочие позиции $\{1....n\} \setminus \gamma$ образуют **проверочную** совокупность (просто все прочие позиции).

Утверждается, что если какие-то позиции $\gamma = {j_1....j_k}$ образуют информационную совокупность, то матрица, составленная из столбцов $j_1....j_k$ порождающей матрицы, обратима.

В каждом кодовом слове позиции, одонозначно определяющие значения символов одни и те же. Информационная совокупность - свойство кода.

Пусть $G(\gamma) = M(\gamma)G$ - порождающая матарица, содержащая единичную подматрицу на столбцах $\gamma$, где $M(\gamma)$ - подходящая обратимая матрица.

ИС свободна от ошибок, если соответствующие позиции вектора $e$ равны 0: $e(\gamma) = 0$.

Декодирование $y = xG + e$ по информационным совокупностям:

- (первоначальный кандидат) $c = 0$;
- Выбрать ИС $\gamma$. Вычислить $c' = y(\gamma)G(\gamma)$;
- Если $d(c', y) < d(c, y)$, то $c = c'$;
- Перейти к следующей ИС. Если все ИС проверены, вернуть $c$;
- Не всякие $k$ позиций образуют информационную совокупность.

Перебирать все информационные совокупноксти необязательно. Если мы на каком-то шаге мы нашли кодовое слово, расстояние которого меньше, чем ($d/2$), то лучшего кодового слова мы не найдем, на этом можно остановиться.

Для коротких кодов можно заранее построить список ИС, достаточный для исправления всех конфигураций ошибок.

Для длинных кодов целесообразно перебирать ИС, добавляя и удаляя на каждом шаге по одному элементу (экономия на приведении порождающей матрицы к каноническому виду $G(\gamma)$).

