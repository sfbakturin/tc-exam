# Блоковые коды и их параметры. Критерии декодирования и метрики. Границы Хемминга и Варшамова-Гилберта

**Блоковый код** – это способ преобразования блока из $k$ **информационных символов** (часто бит, если алфавит двоичный) в блок из $n$ символов (бит), называемый **кодовым словом**.

* Если код двоичный, то на входе имеем $k$ бит $\rightarrow$ на выходе $n$ бит.
* Общее количество разных кодовых слов равно $|C|$. Не путать с размером линейных кодов, где их $2^k$!

Таким образом:

* $\mathbf{n}$ – длина кодового слова (общее число бит).
* $\mathbf{k}$ – число **информационных** бит (входных).
* $\mathbf{M} = |C|$ - число кодовых слов в коде.
* $\mathbf{R} = \dfrac{log_{|X|}M}{n}$ – **кодовая скорость** (доля информационных бит среди всей длины).
* $\mathbf{d}$ – **минимальное кодовое расстояние** (Hamming distance) между любыми двумя разными кодовыми словами.

**Зачем нужна избыточность?**

1. Дополнительные (проверочные) биты помогают **обнаруживать** и **исправлять** ошибки, возникающие при передаче по шумному каналу.
2. Чем выше $\mathbf{d}$ (минимальное расстояние), тем больше ошибок можно исправить. Но при фиксированном $n$ увеличение $\mathbf{d}$ обычно уменьшает $\mathbf{k}$ (а значит, понижает скорость).

**Что такое кодовое расстояние и ошибка?**

* **Hamming distance** между двумя двоичными словами – это число позиций, в которых они различаются.
  * Например, кодовые слова **01001** и **01010** отличаются в двух битах, следовательно, имею расстояние 2.
* **Минимальное расстояние** $d$ – это наименьшая Hamming distance между *любыми* двумя кодовыми словами.
* Если $d \geqslant 2t + 1$, код может **исправлять** до $t$ ошибок (потому что сферы радиуса $t$ вокруг кодовых слов не пересекаются).
* Если $d \geqslant t + 1$, код может **обнаруживать** до $t$ ошибок (но, возможно, не исправлять).

---

**Что значит «декодировать»?** Мы передаём одно из кодовых слов, но на приём приходит искажённая последовательность из $n$ бит.

**Задача**: найти, какое исходное кодовое слово было отправлено (пришло «ближе всего» к полученному вектору).

**Критерии**:

1. Минимального расстояния

Работает только когда $\mathbb{X} = \mathbb{Y}$ (такое чувство, что всегда) тупо подбирает ответ под пришедгий вектор: 

$$\argmin_{c \in \mathcal{C}} d(c, y)$$

2. Списочное декодирование

Подбираем все слова, лежащие в некотором радиусе $r$ от пришедшего

3. Побитовое декодирование

Пытаемся принимать решение о каждом бите отдельно, но с учётом пришедшего вектора (критерий идеального наблюдателя).

$$L_i = \frac{\ln \left(\sum_{c \in \mathcal{C}; c_i = 0} P\{c | y\}\right)}{\ln \left(\sum_{c \in \mathcal{C}; c_i = 1} P\{c | y\}\right)}$$

**Метрики:**
1. Хэмминга

$$d_H(x, y) = \left|\{i \, | \, x_i \neq y_i\}\right|$$

Для двоичного симметричного канала $(p < 0, X = Y = \{0, 1\})$ эта метрика <u>сводит критерий максимального правдоподобия к критерию минимального расстояния</u>
2. Евклида

$$d_E(x, y) = \sqrt{\sum_i^n (x_i - y_i)^2}$$

<u>Same</u> для 2-АМ с АГБШ ($Y = \mathbb{R}^n$) (?)

3. Ли

$$ d_L = \sum_i^n \min\left(|x_i - y_i|, q - |x_i - y_i|\right)$$

$A = GF(q)^n$

<u>Same</u> для канала с q-ичной фазовый модуляцией и АГБШ (?)

4. Ранговое

$$d_R(x, y) = \text{rank}(x - y)$$

$A = GF(q)^{n \times m}$

**Метрики и критерии (GPT?)**.

1. **Минимальное Hamming distance** (для двоичного симметричного канала):
   1. Считается, что каждая позиция «переворачивается» с вероятностью $p$.
   2. Критерий **максимального правдоподобия (ML)** в этом случае сводится к тому, чтобы выбрать кодовое слово, которое отличается от принятой последовательности в **минимальном** числе бит.

2. **Минимальное евклидово расстояние** (если сигнал передавался в аналоговой форме, например BPSK + гауссовский шум):
   * Нужно найти, чья «амплитуда/фаза» ближе к принятому вектору.
   * При гауссовском шуме «ближайшее в евклидовом смысле» = «ML-декодирование».

**Линейные коды и синдромное декодирование**.

* В практике широко применяют **линейные** коды. Порождающая матрица $G$ размера $n \times k$ умножается на вектор $\mathbf{u}$ длины $k$, получаем $\mathbf{c}$ (кодовое слово) длины $n$.

* На приёмной стороне считают **синдром** $\mathbf{s} = \mathbf{r}\,H^\top$, где $H$ - проверочная матрица размера $(n - k) \times n$. Умножение происходит по модулю 2, то есть на выходе получаем бинарный вектор длины $n - k$. Если $\mathbf{s} = 0$, ошибок нет (или они мимикрировали так, что не заметны). Если $\mathbf{s}\neq 0$ – он указывает возможную позицию (или паттерн) ошибки. Например, $s = 011$, тогда мы инвертируем бит на 3-й позиции слева. Таким образом можем исправить до 1-й ошибки.

---

Чтобы понять, **какие** комбинации $(n,k,d)$ вообще достижимы, изучают специальные **границы**:

1. **Граница Хемминга (sphere packing bound)**.
   * Говорит, что «шары» радиуса $t = \lfloor \frac{d-1}{2} \rfloor$ (получили из того самого неравенства $d \ge 2t + 1$) вокруг кодовых слов не должны перекрываться во всём пространстве из $2^n$ бит.
   * Другими словами, у нас есть кодовые слова длины $n$, $k$ битов которых - информационные, остальные - проверочные.. Тогда всего таких векторов $2^n$, то есть $2^n$ точек в $n$-мерном пространстве. Отметим эталонные вектора - вектора, которые к нам могут придти, мы их точно знаем заранее - в нашем пространствен из $2^n$ точек. Тогда, чтобы однозначно определить, к какому эталонному вектору относится тот, что пришёл к нам в декодер, мы, зная некоторую область принадлежности вокруг каждой точки / вектора, должны, отметив пришедший вектор в этом пространстве, попасть ровно в **одну** такую область. То есть эти области должны не пересекаться.

   * Формально:

        $$ 2^k \cdot \sum_{i=0}^{t} \binom{n}{i} \;\le\; 2^n, $$

        где $2^k$ - всего информационных векторов, $2^n$ - количество точек в пространстве, которое мы не можем превысить, $t$ - количество ошибок, которое мы можем исправить, $\binom{n}{i}$ - количество способов выбрать $i$ элементов в векторе длины $n$
   * Ещё более формально (от Трифонова):
      Для любого $q$-ичного кода с $d = 2 t + 1$ выполнено:

      $A_q(n, d) \le \frac{q^n}{\sum_{i = 0}^t C_n^i (q - 1)^i}$

      где $A_q$ --- число кодовых слов
   * Это **верхняя** оценка (если неравенство не выполнено, такой код не может существовать).

2. **Граница Варшамова–Гилберта (Gilbert–Varshamov bound)**.

   * Даёт **нижнюю** оценку: при больших $n$ существуют коды, которые имеют достаточно большое $d$ и при этом достаточно большую $k$.  

   * Упрощённо: существует код, если

        $$ \sum_{i=0}^{d-2} \binom{n-1}{i} < 2^{n-k} $$

        Говорит: «точно найдём код, если не требовать слишком плотной упаковки».
   * По Трифонову: 
      Существует q-ичный код длины n с минимальным расстоянием d, для которого верно:

      $A_q(n, d) \geq \frac{q^n}{\sum_{i = 0}^{d - 1}C_n^i(q - 1)^i}$

В итоге «реальные» коды (например, Хемминга, БЧХ, Рида–Соломона и т.д.) стремятся **улучшать** параметры $(n, k, d)$, не выходя за эти теоретические пределы.
