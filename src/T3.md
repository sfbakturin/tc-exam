# Линейные коды. Границы Синглтона, Варшамова-Гилберта и Грайсмера. Вероятность ошибки декодирования и необнаружения ошибки

## Линейные коды

### Определение линейных кодов

Хранить абсолютно все возможные кодовые слова в памяти не представляется возможным (их может быть например $2^{500}$). Чтобы исправить эту проблему нам нужны более эффективные и компактные способы задания кода. Здесь на помощь приходят линейные блоковые коды.

**Линейным $(n, k)_q$ кодом $C$** называется $k$-мерное подпространство $n$-мерного линейного пространства над полем $GF(q)$ (конечное поле Галуа содержащее $q$ элементов).

Число кодовых слов в таком коде можно найти просто: это есть $q^k$, k - размерность кода, $k / n$ - скорость кода.

Если у нас есть линейное пространство, то у нас есть базис. Давайте выпишем все элементы нашего базиса в отдельную матрицу и назовем ее $G$. Получим $k \times n$ порождающую матрицу линейного кода.

---

**Порождающей матрицей** $k \times n$ $G$ полного ранга, называется такая матрица, что содержит в виде строк все элементы нашего базиса так, что $C = \{y = xG|x \in GF(q)^k\}$.

Иными словами порождающая матрица это такая матрица содержащая в виде строк все элементы базисаа нашего линейного пространства, что все кодовые слова могут быть получены перемножением векторов $x$ на порождающую матрицу.

---

> У всякого линейного пространства $A \subset \mathbb{F}^n$ существует связанное *ортогональное* $\overline{A} \subset \mathbb{F}^n$ такое, что $\forall a \in A, \overline{a} \in \overline{A}: \,\, \left(a, \overline{a}\right) = 0$. Такое пространство существует и для пространства линейных кодов, задаваемого порождающей матрицей $G$, и задаётся оно *проверочной матрицей*.

**Проверочной матрицей** $r \times n$ называется такая матрица $H$, что $C = \{y \in GF(q)^n|yH^T = 0\}, n - k = \text{rank}(H) \leq r$

Иными словами, произведение кодового слова, на транспонированную проверочную матрицу должно равняться нулю. Код можно определить как множество векторов таких, что при умножении на проверучную матрицу у нас получается 0. Или же можно определить **кодовые слова** как всякие разные линейные комбинации строк матрицы $G$. На деле определения абсолютно эквивалентны.

Число строк проверочной матрицы должно быть не меньше чем ранг матрицы $H$ (что то же самое, что и $n - k$).

Порождающая матрица связана с проверочной таким соотношением:

$$GH^T = 0$$

Нужно понимать, что всякий линейный блоковый код имеет множество своих проверочных и порождающих матриц, что может быть полезно для тех или иных задач.

**Эквивалентными кодами** называются такие кода, что один код можно получить перестановкой символов другого кода или домножением на ненулевые константы.

С помощью элементарных линейных операций над строками и перестановкой столбцов порождающая матрица может быть приведена к *каноническому виду* $G = (I | A)$. 
То есть у нас есть какая-то единичная матрица в начале, и что-то в конце.

Такая запись хороша тем, что если мы захотим получить закодированное слово описанным выше способом то мы получим следующую картину:

$$xG = (x|xA)$$

То есть наш информационный вектор $x$ является подвектором кодового слова. Такое кодирование называется **Систематическим кодированием**. Оно упрощает декодирование.

Также у матрицу $H$ можно найти следующим преобразоваанием при такой записи G:

$$H = (A^T|-I)$$

### Свойства линейных кодов

* **Минимально расстояние блокового кода** $C$ равно ${d = \min\limits_{\substack{c' \neq c'' \\ c', c'' \in C}} } d (c', c'') = \min\limits_{c \in C \setminus \{0\}}wt(c)$
То есть минимальное расстояние блокового кода раавно минимальному весу ненулевого кодового слова. (Вес - тупо сумма всех битов). $d$ здесь - расстояние Хемминга

* Если $H$ - проверочная матрица кода длин n, то код имеет размерность $n - r$ тогда и только тогда, когда существуют $r$ линейно независимых столбцов матрицы $H$, а любые $r + 1$ столбцов линейно зависимы. То есть если матрица $H$ имеет ранг $r$.
  
* Если $H$ - проверочная матрица кода длины $n$, то код имеет минимальное расстояние $d$ тогда и только тогдаа, когда любые $1.2.....d-1$ столбцов $H$ линейно незаависимы, но существуют $d$ линейно зависимых столбцов матрицы $H$.

    Что значит, что код имеет минимальное расстояние $d$? Это означает, что в нем есть нулевое кодовое слово, есть кодовое слово веса $d$, но приэтом кодовые слова веса $1 .. d-1$ отсутсвуют. Как бы мы не тужились, состваить взвешенную сумму из $d-1$ столбцов проверочной матрицы, чтобы она была равна нулю нам неудасться. А значит у нас сузествует $d - 1$ линейно независимых столбцов. (кодовых слов нет, а следовательно нет и линейно зависимых столбцов)

    Ну и вторая часть, почему у нас существует $d$ линейно зависимых столбцов?

    Возьмем слово веса $d$. Посмотрим на соответсвующие столбцы матрицы $H$. Так как это кодовое слово, то $dH^T$ = 0, а следоваательно и соотвествующие столбцы матрицы $H$ будут линейно зависимы.

    Из этого следует утверждение, что принадлежность коду ($yH^T = 0$) эквивалентна ЛЗ столбцов. Потому что мы можем подобрать такую комбинацию наших столбцов в $H$ чтобы получить 0, а следовательно и столбцы наши линейно зависимы. (аналогично и в верхнем тейке про минимальное расстояние).

---

## Границы Синглтона

### Верхняя граница

Для любого линейного $(n, k, d)$ кода, $d - 1 \leq n - k$ 

(ранг матрицы $H$ (максимальное число ЛНЗ столбцов) не может превосходить $n - k$).

### Для произвольных кодов

$$A_q(n, d) \leq q^{n - d + 1}$$

То есть для произвольного кода она гласит, что число кодовых слов $q$-ичного кода длины $n$ с минимаальным расстоянием $d$ не превосходит $q^{n - d + 1}$.

Коды, у которых $n - k = d - 1$ называаются разделимыми кодами с максимальным достижимым расстоянием (МДР).

## Вероятность ошибки декодирования и необнаружения ошибки

Насколько хорошо может работать декодер?

Чтобы это понять, нужно ввести понятие весового спекнтр кода:

**Весовой спектр кода** называется такая величина $A_i = |{c \in C|wt(c) = i}|$. То есть каждое $A_i$ - число кодовых слов веса $i$.

Рассмотрим двоичный симметричный канал с переходной вероятностью $p$.

Какова вероятность **необнаружения ошибки**?

Сделать это можно по следующей формуле:

$$P_{\text{undetect}} = P\{S = 0\} = \sum^n_{i=d}A_ip^i(1-p)^{n-i} \leq \sum^n_{i=d}C^i_np^(1-p)^{n-i}$$

По сути это вероятность того, что канал прошумит каким-то кодовым словом. Чтобы найти такую вероятность, нужно перебрать все ненулевые кодовые слова, и вычислить вероятность того, что произошло $i$ ошибок, и что не произошло $n - i$ ошибок и умножить это на число кодовых слов веса $i$ (то есть на наш весовый спектр кода). Нахождение целого спектра кода это задача достаточно нетривиальная, поэтому обычно оценивают по правой границе эту формулу (которая в цетре) ограничивают просто формулой которая справа. Это можно сделать благодаря тому, что число кодовых слов веса $i$ не может быть больше чем число двоичных векторов веса $i$.

**Какова вероятность того, что мы праавильно продекодируем?**

Это вероятность того, что вектор ошибки является лидером смежного класса:

$$P_{\text{correct}} = \sum^l_{i=0}L_ip^j(1 - p)^{n-i}$$

Чтобы найти такую вероятность нужно перебрать всевозможные числа i и посмотреть, сколько у нас есть лидеров смежных классов $i$ и умножить на вероятность того, что произойдет ошибка веса $i$.

То есть в этой формуле:

* $L_i$ - число лидеров смежных классов веса $i$
* l - максимальный вес лидера смежного класса.
* Для совершенных кодов $l = [(d - 1)/2]*L_i = C^i_n$
* В общем случае

$$P_{\text{correct}} \geq \sum^l_{i=0}C^i_np^i(1-p)^{n-i}$$

---

### Напоминание

Лидером смежного класса нызывается Вектора наименьшего веса, соотвествующий какому-то синдрому, называется лидером смежного класса. Это синий столбец в нашей таблице стандартной расстановки (смотри в следующий билет)

### Примечание

Под количество подразумевается количество векторов в этом синем столбце определенного веса.

---

Теперь поговорим про вероятность ошибки мягкого декодирования по максимуму правдоподобия линейного блокового кода. Предположим что у нас аддитивный гаусовский канал с двоичной модуляцией:

$$y_i = (2c_i - 1) + \eta_i, \quad \eta_i \sim \mathcal{N}(0, \sigma^2)$$

Тогда вероятность ошибки мягкого декодирования может быть вычислена по следующей формуле:
$$P \leq \sum_{i=d}^n A_i Q\left(\sqrt{2i \frac{E_s}{N_0}}\right)
= \sum_{i=d}^n A_i Q\left(\sqrt{2iR \frac{E_b}{N_0}}\right)
= \frac{1}{2} \sum_{i=d}^n A_i \operatorname{erfc}\left(\sqrt{iR \frac{E_b}{N_0}}\right)$$

с помощью такой формулы можно оценить сверху вероятность ошибки мягкого декодирования сверху.

## Граница Варшамова-Гилберта

Существует $q$-ичный код длины $n$ с минимальным расстоянием $d$ число слоев которого удовлетворяет $A_q(n, d) \geq \frac{q^n}{\sum^{d-1}_{i = 0} C^i_n(q-1)^i}$

Приэтом существует теорема такая как **граница Хемминга**, которая говорит, что для любого $q$-ичного кодаа с минимальным расстоянием $d = t2 + 1$ число кодовых слов удовлетворяет

$$A_q(n,m,d) \leq \frac{q^n}{\sum^t_{i=0}C^j_n(q-1)^i}$$

Для линейных кодов теорема выглядит следующей:

Если выполняется
$$q^r > \sum^{d-2}_{i=0}C^i_{n-1}(q-1)^i$$

 то существует линейный код над GF(q) длины n с минимальным расстоянием не менее $d$ и не более чем $r = n - k$ проверочными символаами

## Граница Грайсмера

Говорит о том, что если $N(k, d)$ - минимальная длина двоичного линейного кода размерности k с минимальным расстоянием $d$, то тогда выполняется следующее условие:

$$N(k, d) \geq d + N(k - 1, [d /  2])$$
