# Структура систем передачи информации. Простейшие методы модуляции. Критерии идеального наблюдателя и максимума правдоподобия. Вероятность ошибки сигналов 2-АМ в случае канала с АБГШ. Отношение сигнал/шум на бит и на символ

Структура систем передачи информации:

* **Источник** сообщения
* **Кодер**, который добавляет исходному сообщению избыточности для исправления ошибок
* **Модулятор** преобразует цифровой сигнал в аналоговый
* **Канал** для передачи закодированного сообщения
* **Демодулятор** преобразует сигнал обратно из аналогового в цифровой, пытается отделить полезный сигнал от шума, а декодер уже исправляет битовые ошибки. Но логика примерно такая: *демодулятор* восстанавливает "биты"/"символы" из непрерывного аналога, а *декодер* работает с этими битами/символами и исправляет возможные ошибки.
* **Декодер** расшифровывает полученное сообщение и исправляет ошибки
* **Приёмник** принимает декодированное сообщение

---

К простейшим методам модуляции относятся *2-АМ* - двухамплитудная модуляция. Её реализациями являются:

* ASK - модуляция сигнала с помощью амплитуды
* FSK - модуляция сигнала с помощью частоты
* PSK - модуляция сигнала с помощью фазы, это начальное смещение сигнала в периоде синусоиды. Например, синус $\sin(\omega t)$ и $\sin(\omega t + \pi)$ отличаются именно фазой на $\pi$.

Примером простейшей модуляции также является следующая структура:

Пусть передаваемый сингал у нас равен:
$x(t) = \sum_i S_{x_j}(t - iT)$

Где: 
* $i$ - просто порядковый номер передаваемого значения
* $T$ - временная продолжительность символного интервала

А $S_{x_j}$ равна следующему:
$$S_i(t) = \alpha(2i + 1 - M )g(t)sin(2\pi ft)$$

Это называется $M$-ичной амплитудно-импульсной модуляцией. 

Здесь
* g(t) - сигнаальный импульс, какая-то функция, которая говорит о том, сколько будет длиться наш сигнал и сколько значений мы можем передать. 
* f - несущая частота сигнала. Например за нее боряться операторы мобильной связи, чтобы передавать свой сигнал большему числу пользователей
* $\alpha$ - коеффицент, определяющий энергию передаваемого сигнала. 

Имея все это теперь, мы можем получить простую модель канала:

* Для непрерывного времени: $y(t) = x(t) + \eta(t)$
* Для дискретного времени: $y_i = \alpha(2x_i + 1 - M) + \eta_i$
* $\eta_i \backsim N(0, \sigma^2)$ - канал с аддитивным белым гауссовским шумом
$\eta$ - моделирует реальные условия. Когда мы передаем данные у нас появляется какой-то шум. 
---

Задачей декодера является нахождение оценки $\hat{u}$ сообщения, закодированного кодером и подвергшегося случайным искажениям в канале. В случае цифровых сообщений естественным критерием качества декодирования является вероятность ошибки, то есть несовпадение оценки и истинного сообщения $u$. Вероятность такого события равна

$$
  P_c = \Sigma_{u' \ne \hat{u}}P_{U|Y}\{u'|y\} = 1 - P_{U|Y}\{\hat{u}|y\},
$$

где $y$ - последовательность принятых символов. Очевидно, что минимальная вреоятность ошибки достигается при выборе:

$$
  \hat{u} = \operatorname*{arg\,max}_u P_{U|Y}\{y|y\}
$$

Данное правило принятия решения носит название критерия **идеального наблюдателя**. **«Идеальный наблюдатель»** называют так, потому что он «идеально» знает все вероятностные законы канала и сигнала.Из теоремы Байеса вытекает, что

$$
  P_{U|Y}\{u|y\} = \frac{P_{Y|U}\{y|u\}P_U\{u\}}{P_Y\{y\}}
$$

Знаменатель этой дроби от $u$ не зависит и может не учитываться при максимизации. Кроме того, при наличии качественно реализованного кодера источника все сообщения могут считаться равновероятными. В этом случае критерий идеального наблюдателя эквивалентен критерию **максимального правдоподобия**:

$$
  \hat{u} = \operatorname*{arg\,max}_u P_{Y|U}\{y|u\}
$$

---

Прежде всего, **АБГШ** это

* **А**ддитивный, то он прибавляется к исходному сигналу
* **Б**елый, то есть все частотные компоненты равномерны (спектр шума плоский (у шума одинаковая «сила» на всех частотах)).
* **Г**ауссовский, то есть шум подчиняется нормальному гауссовскому распределению с нулевым средним (то есть если мы возьмём отсчёт шума в некоторый момент времени, это будет случайная величина $n$ со средним 0 и дисперсией $\sigma^2$; то есть $n \sim \mathcal{N}(0,\sigma^2)$)
* **Ш**ум - что-то лишнее.

В случае **2-АМ** (два уровня амплитуды: -A и +A, или «0» и «A») при наличии гауссовского шума решение принимается по «пороговому правилу». Например, если принятое значение $r \ge 0$ — решаем «+A», если $r < 0$ — решаем «-A».

* Если шум невелик, $r$ почти всегда будет ближе к правильному уровню, и ошибка случается редко.
* Если шум «большой», ошибка случается часто (перекидывает $r$ за порог).

При АБГШ *вероятность ошибки в двоичной модуляции* обычно выражается через $Q$-функцию.

---

Рассмотрение отношения сигнала/шум на бит и на символ.

* Сигнал/шум на бит = $\dfrac{E_b}{N_0}$, где
  * $E_b$ - энергия, которую мы тратим на передачу одного бита информации
  * $N_0$ - спектральная плотность шума ( #TODO тут я не понял ни слова, я не знаю что такое "спектральный" и не знаю что такое "плотность шума")
    * Это отношение крайне удобно для сравнения эффективности разных систем. Например, у нам могут быть разные модуляции, разные скорости передачи, но сколько энергии уходит на один быть и какова мощность шума универсально характеризует надёжность
* Сигнал/шум на символ = $\dfrac{E_s}{N_0}$, где
  * $E_s$ - энергия на символ. Если в одно символе кодируем $m$ бит, то $E_s = m \cdot E_b$. Тогда $\dfrac{E_b}{N_0} = m \cdot \dfrac{E_s}{N_0}$.
