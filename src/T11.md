# Производящая функция сверточных кодов. Вероятность ошибки декодирования сверточных кодов с помощью алгоритма Витерби

**Производящая функция** сверточных кодов отражает число кодовых слов различного веса.

* *Вероятность ошибки декодирования кода определяется числом кодовых слов различного веса*. Это означает, что вероятность ошибки при декодировании зависит от количества кодовых слов (последовательностей бит), которые имеют одинаковый "вес" — количество единиц в последовательности. Чем больше таких кодовых слов, тем выше вероятность ошибки.
* *Число путей в решетке, начинающихся и заканчивающихся в нулевом состоянии*. Сверточные коды можно представить как граф или решетку состояний. Путь в этой решетке начинается и заканчивается в "нулевом состоянии" — начальном состоянии кодера. Количество таких путей связано с количеством возможных кодовых слов.

Построение производящей функции:

* Пометим ребра графа переходов метками $D^i$, где $i$ – вес выходной последовательности. В графе переходов каждое ребро (переход между состояниями) помечается степенью $D^i$, где $i$ — количество единиц (вес) в выходной последовательности, связанной с этим переходом.
* Последовательность символов веса $x$ характеризуется одночленом $D^x$.
* Совокупность кодовых слов характеризуется многочленом, например $2D^6 + 3D^8$. Здесь многочлен $2D^6 + 3D^8$ означает, что есть $2$ кодовых слова с весом $6$ и $3$ кодовых слова с весом $8$.
* Расщепим исходное состояние на два: начальное (0) и конечное (e).
* Пусть $X^i$ характеризует совокупность кодовых последовательностей, приводящих кодер в состояние $i$. Тогда, $X^i$​ – ряд, коэффициенты которого равны числу кодовых последовательностей, начинающихся в нулевом и заканчивающихся в $i$-ом состоянии.
* Наконец, производящая функция $T(D) = \dfrac{X_{e}}{X_{a}}$. Производящая функция $T(D)$ описывает общую структуру кодовых слов и вычисляется как отношение рядов $X_e$​ (которые заканчиваются в конечном состоянии) к $X_a$ (которые начинаются в начальном состоянии). Это степенной ряд, где каждый коэффициент указывает на количество кодовых слов определённого веса, возвращающихся в нулевое состояние.

Пример построения производящей функции. Дан сверточный код в виде конечного автомата.

![Пример сверточного кода](./assets/11_1.png)

Выразим для него производящую функцию:

$$
  \begin{cases}
    X_b = D^2X_a + X_d \\
    X_c = DX_b + DX_c \\
    X_d = DX_c + DX_b \\
    X_e = D^2X_d
  \end{cases} \implies T(D) = \dfrac{D^5}{1 - 2D} = D^5 + 2D^6 + 4D^7 + 8D^8 + \ldots
$$

Свойства производящей функции:

1. Степень нулевого члена — минимальное свободное расстояние кода.
2. Можно доказать, что минимальное свободное расстояние пропорционально длине кодового ограничения.

---

**Расширенная производящая функция** — производящая функция, построенная по указанному выше алгоритму, но вместо метки $D^i$ на ребрах используется метка $D^iN^j$, причем $j$ равен биту перехода. В итоге получается степенной ряд от переменных $N$ и $D$, в котором коэффициент при $D^aN^b$ равен число кодовых последовательностей веса $a$, порождаемых информационными последовательностями веса $b$.

---

Вероятность **ошибки** — вероятность того, что будет выбран какой-либо неправильный путь. Если производящая функция имеет вид $T(D) = \displaystyle\sum{\left(t_dD^d\right)}$, то вероятность ошибки:

$$
  \begin{aligned}
    P_e &= P\{(C_1 > C_0) \lor (C_2 > C_0) \lor \ldots \} \leqslant \sum_{i}{\left(P\{C_{i} > C_0\}\right)} \\
        &= \sum_{d > 0}{t_dP_d} \\
        &= \sum_{d = d_{\mathtt{free}}}{\left(t_dQ\left(\sqrt{2dR\dfrac{E_b}{N_0}}\right)\right)}
  \end{aligned}
$$

Здесь:

* $P_e$: Вероятность ошибки декодирования. Это вероятность того, что будет выбран неправильный путь при декодировании.
* $C_0$: Метрика (накопленная ошибка) правильного пути (корректной кодовой последовательности).
* $C_i$: Метрика для неправильного пути $i$. Это метрика для альтернативных путей, которые могут привести к ошибке декодирования.
* $t_d$: Количество путей в кодере, которые имеют метрику $d$. Это число путей, для которых кодовая последовательность отличается от правильной на расстояние $d$.
* $P_d$: Вероятность того, что метрика для пути с расстоянием $d$ больше, чем метрика для правильного пути $P_d = P\{C_i > C_0\}$.
* $Q(\ldots)$: Функция $Q$, используемая для оценки вероятности того, что гауссовский случайный процесс превысит определённое значение. Она применяется для оценки вероятности ошибки из-за шума.
* $R$: Скорость кода, равная отношению числа информационных бит к общему числу переданных бит $R = \dfrac{k}{n}$.
* $\dfrac{E_b}{N_0}$: Отношение энергии на бит к спектральной плотности мощности шума, определяющее уровень шума в канале связи.

Пояснение формулы:

* $P\{(C_1 > C_0) \lor (C_2 > C_0) \lor \ldots \}$: Это вероятность того, что метрика любого неправильного пути $C_i$​ окажется больше метрики правильного пути $C_0$, что приводит к ошибке декодирования.
* Неравенство $P_e \leqslant \sum_{i}{\left(P\{C_{i} > C_0\}\right)}$: Используется неравенство объединения (суммирование вероятностей событий), чтобы получить верхнюю оценку вероятности ошибки декодирования. Каждое слагаемое $P\{C_i > C_0\}$ оценивает вероятность того, что конкретный неправильный путь будет выбран.
* Суммирование по $d>0$: Здесь учитываются все возможные расстояния $d$, которые превышают ноль, то есть все ошибочные пути, которые могут быть приняты вместо правильного.
* Сумма $\sum_{d = d_{\mathtt{free}}}{\left(t_dQ\left(\sqrt{2dR\dfrac{E_b}{N_0}}\right)\right)}$: Каждое слагаемое в этой сумме умножает количество путей с метрикой $d$ на вероятность того, что метрика этого пути превысит метрику правильного пути. Функция $Q$ используется для оценки этой вероятности на основе гауссовского шума.

Эта формула даёт верхнюю оценку вероятности ошибки декодирования $P_e$ сверточного кода при использовании алгоритма Витерби в канале с аддитивным белым гауссовым шумом (АБГШ). Она учитывает количество возможных ошибочных путей и их вероятность, что позволяет оценить общую вероятность того, что декодер выберет неправильный путь. Чем выше $\dfrac{E_b}{N_0}$ и чем больше свободное расстояние $d_{\mathtt{free}}$, тем ниже вероятность ошибки.

Пусть, расширенная производящая функция $T(N, D) = \displaystyle\sum_{w, d}{\left(t_{wd}N^wD^d\right)}$. Тогда, общая вероятность ошибки декодирования на бит выражается следующим образом:

$$
  P_b \leqslant \dfrac{1}{k_0} \sum_{d = d_{\mathtt{free}}}^{\infty}{\left(b_dQ\left(\sqrt{2Rd\dfrac{E_b}{N_0}}\right)\right)}
$$

Здесь:

* $P_b$: Вероятность ошибки на бит. Это вероятность того, что декодированный бит будет неверным.
* $k_0$: Среднее число информационных бит, передаваемых на одну кодовую последовательность. Это нормировочный фактор, который учитывает количество переданных информационных бит.
* $\sum_{d = d_{\mathtt{free}}}^{\infty}$: Суммирование начинается с $d_{\mathtt{free}}$, минимального свободного расстояния кода, и продолжается до бесконечности. Это учитывает все возможные кодовые слова, начиная с минимального расстояния.
* $b_d$: Количество кодовых последовательностей с расстоянием $d$ (весовые коэффициенты). Это весовой спектр кода, который показывает, сколько кодовых слов имеют определённое расстояние $d$.
* $Q(...)$: Функция $Q$, которая даёт вероятность того, что гауссовский случайный процесс примет значение выше заданного. Она используется для оценки вероятности того, что шум вызовет ошибку.
* $R$: Скорость кода, определяемая как отношение числа информационных бит к общему числу переданных бит $R = \dfrac{k}{n}$.
* $d$: Кодовое расстояние между различными кодовыми словами. Чем больше $d$, тем лучше код исправляет ошибки.
* $\dfrac{E_b}{N_0}$: Отношение энергии на бит к спектральной плотности мощности шума. Это ключевой параметр, определяющий уровень шума в канале связи. Чем выше это отношение, тем лучше передача.

Эта формула показывает, как вероятность ошибки на бит $P_b$ зависит от расстояния между кодовыми словами $d$, параметров кода $R$ и $b_d$, и уровня шума в канале $\dfrac{E_b}{N_0}$. Чем больше свободное расстояние $d_{\mathtt{free}}$ и лучше условия передачи - выше $\dfrac{E_b}{N_0}$, - тем ниже вероятность ошибки.


---

*Альтернативное про ошибки от @sn1tr0n:*

Мы хотим сверху оценить вероятность ошибки декодирования в канале с АБГШ.

$$r_{ij} = (-1)^{c_{ij}} + \eta_{ij}, ~\eta_{ij} \sim N\left(0, \sigma^2\right), ~~ j \in [1, n_0], ~~ i \in [0, +\inf)$$

Для начала, давайте предположим, что передаётся нулевое слово ($\forall i, j : c_{ij} = 0$). И пусть Витерби работает по метрике максимальной корреляции (тут похоже машем руками, что это тоже критерий максимального правдоподобия)

$$C = \sum_{i \geq 0} \sum_{j = 1}^{n_0} y_{ij}(-1)^{c_{ij}}$$

Оценим вероятность первого события неправильного декодирования (типа мы шли-шли по решётке, декодировали корректно, и в этот момент (на ярусе B) приняли неправильное решение). Это значит, что метрика ненулевого $C_1$ ($c_{ij}$) пути оказалась больше нашей нулевой $C_0$ (напоминаю, что мы передаём нулевое слово).

$$P\{C_1 > C_0\} = P\{\sum_{i = 0}^B \sum_{j = 1}^{n_0} r_{ij}((-1)^{c_{ij}} - 1) > 0\} = \ldots$$

Чистая единичка внутри скобки --- это и есть компонент нашего нулевого слова

$$\ldots = P\{\sum_{i = 0}^B \sum_{j = 1;\,\, c_{ij} \neq 0}^{n_0} r_{ij} < 0 \}$$

Такс, теперь вспоминаем, что мы передавали нулевое слово. Это значит, что $r_{ij} = 1 + \eta_{ij}$ и $r_{ij} \sim N(1, \sigma^2)$. Пусть неправильный пусть имеет вес $d$ на ярусах с $0$ по $B$. Тогда:

$$\rho\sum_{i = 0}^B \sum_{j = 1;\,\, c_{ij} \neq 0}^{n_0} r_{ij} = r_{ij} \sim N\left(d, d\sigma^2\right)$$

Забытые знания (билет 1):

$$\sigma^2 = \frac{N_0}{2}$$

Тогда вероятность ошибочного пути веса $d$:

$$P_d = P\{\rho < 0\} = Q\left(\sqrt{2d\frac{E_s}{N_0}}\right) = Q\left(\sqrt{2d\frac{E_b}{RN_0}}\right)$$

$$Q(x) = \frac{1}{\sqrt{2\pi}}\int_x^{\infty}e^{\frac{-t^2}{2}}dt = \frac{1}{2}\text{erfc}\left(\frac{x}{\sqrt{2}}\right)$$

Осталось только оценить сверху **всю** ошибку (она может произойти на любом пути, нам достаточно одного):

$$P_e = \{(C_1 > C_0) \lor (C_2 > C_0) \lor \ldots\} \le \sum_i P \{C_i > C_0\} = \sum_{d > 0} t_d P_d = \sum_{d = d_{\text{free}}} t_dQ\left(\sqrt{2d\frac{E_b}{RN_0}}\right)$$

Производящая функция: $T(D) = \sum_{d \geq 0}t_dD^d$

---

С ошибкой на бит происходит какая-то шиза. Утверждается, что любой ошибочный путь характеризуется также ещё и весом $w$ информационной (то есть входной) последовательности. И вероятности ошибки для одного пути равна:

$$\frac{w}{k_0}P\{C_i > C_0\} = \frac{w}{k_0}P_d$$

Вспоминаем про расширенную производящую функцию $T(N, D) = \sum_{w, d} t_{wd}N^wD^d$

Что вот тут происходит, я не понял:

$$t(D) = \frac{\partial T(N, d)}{\partial N}\biggr|_{N = 1} = \sum_d D^d \underbrace{\sum_wt_{wd}w}_{}b_d$$

И тогда вероятность ошибки декодирования на бит:

$$P_b \le \frac{1}{k_0}\sum_{d = d_{free}}^{\infty} b_dQ\left( \sqrt{2Rd\frac{E_b}{N_0}}\right)$$

