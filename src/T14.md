# Параметр Бхаттачарьи. Поляризация канала. Полярные коды. Сложность кодирования

## Напоминание про функции переходных вероятностей канала

Для полярных кодов мы буде рассматривать канал без памяти со входным алфавитом $X = \{0, 1\}$

Будем считать, что выходной алфавит канала ($Y$) дискретный.

**Для случая с дискретныым выходным алфавитом** $\mathcal{Y}$ :

- $W(y|c)$ — вероятность наблюдения на выходе канала $y \in \mathcal\{Y\}$ при условии подачи на его вход $c \in \mathcal{X}$.

- **Двоичный симметричный канал**: $\mathcal{Y} = \mathcal{X}$,  
  $$
  W(y|c) =
  \begin{cases}
  p, & y \neq x, \\
  1 - p, & y = x.
  \end{cases}
  $$

- **Двоичный стирающий канал**: $\mathcal{Y} = \{0, 1, \varepsilon\}$,  
  $$
  W(y|c) =
  \begin{cases}
  p, & y = \varepsilon, \\
  1 - p, & y = x \in \{0, 1\}.
  \end{cases}
  $$

- **Двоичный симметричный канал со стираниями**: $\mathcal{Y} = \{0, 1, \varepsilon\}$,  
  $$
  W(y|c) =
  \begin{cases}
  1 - p - s, & y = x, \\
  s, & y = \varepsilon, \\
  p, & y = 1 - x.
  \end{cases}
  $$

**Для непрервыного выходного алфавита** $\mathcal{Y}$

- $W(y|c)$ — плотность распределения выходного символа канала при подаче $c$ на его вход.

- **Аддитивный гауссовский канал**:  
  $\mathcal{Y} = \mathbb{R}, \; y = (-1)^c + \eta, \; \eta \sim \mathcal{N}(0, \sigma^2)$.  

  Формула вероятностной плотности:  
  $$
  W(y|c) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{\left(y - (-1)^c\right)^2}{2\sigma^2}}.
  $$

Для простоты мы будем считать $\mathcal{Y}$ дискретным.

В принципе, все то, что будет сказано дальше относительно легко пеереноситься на непрерывный выход.

## Параметр Бхаттарчьи

Представляет собой оценку сверху вероятности ошибки на бит для приемника по максимуму правдоподобия.

- рассмотрим **Приёмник по максимуму правдоподобия**. Мы передаем по каналу какие-то символы $c$ и глядя на каждый соотвествующий символ пытаемся определить $\hat{c}$, используя при этом критерий максимум правдоподобия:
  $$
  \hat{c} = \arg \max_{c \in \{0,1\}} W(y|c)
  $$

- **Передаваемые символы равновероятны**.

- **Вероятность ошибки**. Здесь это вероятность ошибки при передаче нуля, и вероятность ошибки при передаче 1. Считается, что 0 и 1 передаются с вероятностью 1/2.
  $$
  P_e = P\{c = 0\} P\{\text{err}|c = 0\} + P\{c = 1\} P\{\text{err}|c = 1\}
  $$
  При равновероятных символах:
  $$
  P_e = \frac{1}{2} \sum_{y : W(y|0) < W(y|1)} W(y|0) + \frac{1}{2} \sum_{y : W(y|1) < W(y|0)} W(y|1)
  $$

  Это можно переписать как:
  $$
  P_e = \frac{1}{2} \sum_{y \in \mathcal{Y}} \sum_{c \in \{0,1\}} W(y|c) \chi \left( \frac{W(y|1-c)}{W(y|c)} \right),
  $$
  где $\chi(z)$ — индикаторная функция.

- **Индикаторная функция**:
  $$
  \chi(z) =
  \begin{cases}
  1, & z \geq 1, \\
  0, & z < 1.
  \end{cases}
  $$
  Кроме того, выполняется неравенство:
  $$
  \chi(z) \leq \sqrt{z}.
  $$

- **Оценка сверху для вероятности ошибки**:
  $$
  P_e \leq \frac{1}{2} \sum_{y \in \mathcal{Y}} \sum_{c \in \{0,1\}} W(y|c) \sqrt{\frac{W(y|1-c)}{W(y|c)}}.
  $$

- Обозначим:
  $$
  Z(W) = \sum_{y \in \mathcal{Y}} \sqrt{W(y|0) W(y|1)}.
  $$

  Тогда:
  $$
  P_e \leq Z(W).
  $$

 $Z(w)$ - и есть функция, которая называется параметром **Бхаттачарьи**

 Например, для двоичного стирающего канала параметр Бхттачарьи будет равен следующему:

$$
Z(BEC(p)) = \sqrt{W(0|0)W(0|1)} + \sqrt{W(1|0)W(1|1)} + \sqrt{W(\varepsilon|0)W(\varepsilon|1)} = p.
$$

## Пропускная способность канала

Пропускная способность канала \( W \) выражается через взаимную информацию следующим образом:

$$
I(W) = \max_{\{P(x)\}} \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} W(y|x) P(x) \log \frac{W(y|x)}{W(y)},
$$

где:

- $\mathcal{X}$ — входной алфавит,
- $\mathcal{Y}$ — выходной алфавит,
- $W(y|x)$ — вероятность перехода канала,
- $P(x)$ — распределение вероятностей входного символа,
- $W(y)$ — апостериорная вероятность выхода $y$, вычисляемая как:
  $$
  W(y) = \sum_{x \in \mathcal{X}} W(y|x) P(x).
  $$

Пропускную способность канала нужно максимизировать по распределению входов канала.

Существуют методы сколь угодно надежной передачи данных со скорость $R < I(W)$

При передче данных со скорость $R > I(W)$ вероятность ошибки ограничена снизу положительной величиной.

Для многих каналов оптимальным распределением символов на входе $P\{x\}$ является равномерное.

## ПОЛЯРИЗАЦИЯ КАНАЛА

Рассмотрим линейное преобразование, задаваемое следующей штукой:

![Поляризация канала](./assets/14_1.png)

У нас было 2 бита данных $u_0$ и $u_1$ и мы умножжили их на такую матрицу и получили биты $c_0$ и $c_1$:
$$
\begin{pmatrix}
c_0 & c_1
\end{pmatrix}
=
\begin{pmatrix}
u_0 & u_1
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
1 & 1
\end{pmatrix}
$$

Давайте теперь результат вычислений отправим по двоичному стриающему каналу:

![Поляризация канала](./assets/14_2.png)

Двоичный стирающим каналом мы назваем такую штуку:

$$
y_i =
\begin{cases}
c_i, & \text{c вероятнстью $1 - p$}, \\
\epsilon, & \text{с вероятностью p}.
\end{cases}
$$

В результате этой залупы мы получим какие-то символы $y_0$ и $y_1$

Теперь, глядя на эти символы попытаемся понять чему было равно $u_0$?

Если произошло хотя бы одно стирание, то мы не сможем дать ответ на этот вопрос.

Вот если ничего не стерлось, то мы можем взять и сделать следующую операцию:

$$u_0 = y_0 - y_1$$

Вот так и получаем наше $u_0$

- $u_0$ не может быть восстановлен из $y_0$, $y_1$ с вероятностью $1 - (1 - p)^2 = 2p - p^2 \geq p$
- $u_1$ не может быть восстановлен из $u_0$, $y_0$, $y_1$ с вероятностью $p^2 \leq p$

Предположим теперь, что добрый дядя нам сказал, чему равно значение $u_0$. Теперь зная это, а еще значения $y_0$ и $y_1$ попытаемся оценить значение $u_1$

Что можно сделать? Если $y_1$ не стерт, тогда он просто равен $u_1$.

Второй вариант, если $y_0$ не стерт, то

$$u_1 = y_0 - u_0$$

Есть 2 способа восстановить $u_1$. Восстановить мы не можем его только в том случае, если оба символа $y_0$ и $y_1$ оказались стертыми. Вероятность такого события как раз таки и равна $p$.

Исходя из неравенст написанных в непрономированном списке выше, мы можем сделать тот вывод, что при $p$ отличным от единицы мы получили 2 канала, один из которых чуть лучше, другой чуть хуже.

Оказывается, такое преобразование можно проделать несколько раз. Посмотрим на визуализацию примера для двоичного стирающего канала с $p = 0.5$

![Поляризация канала](./assets/14_3.png)

Кодер работает снизу вверх, декодер работает сверху вниз.

Внизу мы видим вероятности стирания. то есть, изначально она была везде 0.5, но после первого шага, где-то уже стала 0.75, а где-то 0.25. (как я понимаю, это резальутат применения неравенств которые мы определили для $u_0$ и $u_1$)

то есть на выходе мы получаем какие-то вообще ненадежные каналы, а какие-то совсем надежные, в результате применения такой конструкции.

после того как мы получили такие синтетические каналы возникает вполне естественное желание. Если мы хотим надежно передавать данные, то эти данные нужно передавать по надежным каналам, а по ненадежным передавать не надо.

Как этого добавиться? Подавать на вход фиксированные значения, которые всем известны. Тогда получается например, для каналов которые стирают данные будем передавать всегда нули, а для наших хороших каналов передавать данные по ним. В таком случае если бы мы передавали 4 бита данных, мы бы попали в канал со стиранием не больше чем 0.32 (передаем для $y_0, y_4, y_2, y_6$)

Все эти преобразования они полуобратимы, мы не теряем здесь никакую информацию.

Как это преобразование задается в общем случае?

- Пусть матрица $A_m$ определяется как:

  $$
  A_m =
  \begin{pmatrix}
  1 & 0 \\
  1 & 1
  \end{pmatrix}^{\otimes m},
  $$

  где $\otimes m$ обозначает $m$-кратное произведение Кронекера матрицы с самой собой.

- Пусть $n = 2^m$.

- **Краткая запись подвекторов**:
  $$
  y_a^b = (y_a, y_{a+1}, \dots, y_b).
  $$

- Если у нас есть функция перехода вероятностей для нашего канала, то можем ввести векторную функцию переходных вероятностей:
  $$
  W_m(y_0^{n-1} | c_0^{n-1}) = \prod_{i=0}^{n-1} W(y_i | c_i).
  $$

- **Синтетические битовые подканалы**:
  Эти подканалы используются для построения полярных кодов.

Формула для $W_m^{(i)}$, вероятности для $i$-го синтетического подканала:

$$
W_m^{(i)}\left(y_0^{n-1}, u_0^{i-1} \mid u_i \right) = \frac{W_m^{(i)}\left(y_0^{n-1}, u_0^{i}\right)}{P\{u_i\}} =
\sum_{u_{i+1}^{n-1} \in \mathbb{F}_2^{n-i-1}} W_m^{(n-1)}\left(y_0^{n-1} \mid u_0^n\right) P\{u_{i+1}^{n-1}\}.
$$

После упрощений:

$$
W_m^{(i)}\left(y_0^{n-1}, u_0^{i-1} \mid u_i \right) =
\frac{2}{2^n} \sum_{u_{i+1}^{n-1} \in \mathbb{F}_2^{n-i-1}} W_m\left(y_0^{n-1} \mid u_0^{n-1} A_m\right).
$$

Финальная форма:

$$
W_m^{(i)}\left(y_0^{n-1}, u_0^{i-1} \mid u_i \right) =
2^{-n+1} \sum_{u_{i+1}^{n-1} \in \mathbb{F}_2^{n-i-1}} \prod_{j=0}^{n-1} W\left(y_j \mid (u_0^{n-1} A_m)_j\right),
$$

где:

- $A_m$ — матрица Кронекера
- $u_0^{n-1}$ — последовательность информационных бит.

![Битовые подканалы](./assets/14_4.png)
Здесь изображено трехкратное произведение матрицы самой с собой.

### Функция переходных вероятностей битовых подканалов

![Битовые подканалы](./assets/14_5.png)

### рекурсивное определение подканалов

![Битовые подканалы](./assets/14_6.png)

### параметры подканалов

![Битовые подканалы](./assets/14_7.png)

### визуализация поляризации канала

![Битовые подканалы](./assets/14_8.png)

для m = 4 получилось 6 или (37.5%) недополяризованных подканалов

для m=10 получилось 94 (9.2%) недополяирозованных подканалов

- доля недополяризованных подканалов убывает с увеличением m
- Число недополяризованных подканалов растет с увеличением m

## полярные коды и сложность декодирования

![Полярный код и алгоритм последовательного исключения](./assets/14_9.png)

Что делаем? По плохим каналам передаем нули, по хорошим каналам передаем полезные данные.

На рисунке изображена схема, которая соотвествует декодеру длины 8 с размерностью 4. В данном конкретном случае получается расширенный код Хемминга.

А вот так происходит кодирование:

$$c^{n-1}_0 = u^{n-1}_0A_m, u_i = 0, i\in \mathcal{F}$$

 где $\mathcal{F}$ - множество номеров плохих подканалов (замороженных символов)

В итоге кодирования у нас получается линейный блоковый код ($2^m, 2^m - |\mathcal{F}|$)

### Алгоритм последовательного исключения

Это про то как рботает декодер.

Декодер последовательно оценивает значения всех $\hat{u_i}$. Те, которые заморожены мы ставим им 0, которые не заморожены принимаем решение по максимуму правдоподобия.

$$
\hat{u}_i =
\begin{cases}
0, & i \in \mathcal{F}, \\
\arg \max_{u_i} W_m^{(i)}\left(y_0^{n-1}, \hat{u}_0^{i-1} \mid u_i\right), & i \notin \mathcal{F},
\end{cases}
$$

где:

- $\mathcal{F}$ — множество замороженных битов (известные значения, часто фиксируются на \( 0 \)),
- $W_m^{(i)}$ — вероятность для \( i \)-го синтетического подканала,
- $y_0^{n-1}$ — принятые символы,
- $\hat{u}_0^{i-1}$ — уже декодированные значения предыдущих битов.

Если предыдущие решения были правильными, то $\hat{u}^{i-1}_0 = u^{i-1}_0$

Если ранее была допущена ошибка, то алгоритм последовательного исключения все равно не сможет исправить.

Если строиться код с пропускной способностью близкой к пропускной способности канала, то вероятность ошибки будет такой:

$$
P \leq \sum_{i \notin \mathcal{F}} Z_{m, i} \leq 2^{-n^\beta}, \quad \beta < 0.5,
$$

где:

- $\mathcal{F}$ — множество замороженных битов,
- $Z_{m, i}$ — мера качества \( i \)-го синтетического подканала,
- $n = 2^m$ — длина кода,
- $\beta$ — параметр, определяющий скорость поляризации подканалов.

Выше один из вариантом алгоритма последоватльного исключения.

Также используя логарифмическое отношение праводоподобия полярных кодов можно получить следующий алгоритм:

$\text{For } i = 0, 1, \ldots, 2^m - 1: \hat{u}_i =
\begin{cases}
0, & i \in \mathcal{F}, \\
0, & L_m^{(i)}(y_0^{n-1}, \hat{u}_0^{i-1}) > 0, \ i \notin \mathcal{F}, \\
1, & L_m^{(i)}(y_0^{n-1}, \hat{u}_0^{i-1}) \leq 0, \ i \notin \mathcal{F}.
\end{cases}$

Логарифмическим отношением правдоподобия мы называем следующую формулу:

$$
L_m^{(i)}(y_0^{n-1}, u_0^{i-1}) = \ln \frac{W_m^{(i)}(y_0^{n-1}, u_0^{i-1} \mid 0)}{W_m^{(i)}(y_0^{n-1}, u_0^{i-1} \mid 1)}
$$

А это конструкция к которой это все сводиться:

$$
L_\lambda^{(2i+1)}(y_0^{n-1}, u_0^{2^i}) = (-1)^{u_{2^i}} L_{\lambda-1}(y_0^{2^\lambda-1, even}, u_0^{2^i-1, even} + u_0^{2^i-1, odd})
+ L_{\lambda-1}(y_0^{2^\lambda-1, odd}, u_0^{2^i-1, odd})
$$

Полярные коды можно починить (у них по итогу выходит не очень большое минимальное расстояние, что дает нам такую-себе способность чет декодиировать, например у кодов Рида-Маллера, которые частный случай полярных кодов она гораздо лучше)

Чтобы исправить это нам нужно разобраться с причинами неудовлетворительной корректирующей способности.

Если мы возьмем алгоритм последоветельного исключения, он последовательно принимает решения об каждом из символов. Приэтом при принятии решения относительно очередного символа он никак не учитывает информацию о том, что какие-то последующие символы могут быть заморожены. В следствеи чего принимаемые им решения оказываются субоптимальными. Если мы приняли такое решение, дальше мы исправить его не можем. Он не обеспечивает декодирование полярных кодов по максимуму правдоподобия.

Это можно исправить **списочным декодированием**.

Мы можем принимать окончательное решение относительно $u_i$ на фазе $u_i$. Мы представляем, что наши вектора $u$ представляют собой пути в каком-то дереве.

На каждой фазе $i$ будем рассматривать $L$ веткторов $u^{i-1}_0$, строить их возможные продолжения $u^i_{0}$ и выбирать из них $L$ наиболее вероятных.

Вектора $u^i_0$ задают пути в кодовом дереве.

![Списочное декодирование](./assets/14_10.png)

### Сложность декодирования полярного кода

- Разложение матрицы:

$$
u_0^{n-1} A_m =
\begin{pmatrix}
u_0^{n/2 - 1} & u_{n/2}^{n-1}
\end{pmatrix}
\begin{pmatrix}
A_{m-1} & 0 \\
A_{m-1} & A_{m-1}
\end{pmatrix} =
\begin{pmatrix}
(u_0^{n/2 - 1} + u_{n/2}^{n-1}) A_{m-1} & u_{n/2}^{n-1} A_{m-1}
\end{pmatrix}.
$$

По факту это наша задача, мы должны умнжоить этот вектор на матрицу. Эта задача сводиться к двум задачам умножения на матрицу $n / 2$. В итоге получаем рекуретное соотношение и такую сложность:

- Сложность алгоритма:

$$
T(n) = T(n/2) + n/2 = \frac{1}{2} n \log_2 n.
$$

по памяти сложность O(n)

Почему так?

У нас переиспользуются на фазе 2$i$ + 1 сомножителей $W^{(i)}_{\mu - 1}$ вычиисленных на фазе 2i

При обновлении на слое $\lambda$ вычисляются $2^{m-\lambda}$ ЛОПП

сложность будет такой:

$$C = \sum^m_{\lambda = 1} 2^\lambda * 2^{m - \lambda} = m2^m = n\log_2n$$
