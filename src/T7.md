# Критерии мягкого декодирования. Метод порядковых статистик

## Критерии мягкого декодирования

Декодирование кода $C$ по критерию макимуму правдоподобия в канале с АБГШ эквивалентно декодированию по критерию минимального расстояния Евклида.

Можно ли как-то еще сформулировать ту задачу?

Давайте рассмотрим передачу по каналу с АБГШ символов $(-1^{c_i})$, где $c_i \in {0, 1}$, т.е $y_i = (-1)^{c_i} + \eta_i$

Нам нужно декодировать по критерию минимального расстояния Евклида:

$$\arg\min_{c \in C} \sum_{i=0}^{n-1} (y_i - (-1)^{c_i})^2
= \arg\min_{c \in C} \sum_{i=0}^{n-1} (y_i^2 - 2(-1)^{c_i}y_i + (-1)^{2c_i})
= \arg\max_{c \in C} \sum_{i=0}^{n-1} (-1)^{c_i}y_i$$

В итоге получаем, что достаточно лишь максимизировать корреляцию принятого слова с принятым зашумленным вектором.

Попытаемся ее еще переформулировать:

Определим что называется **жестким решением**

Это такие
$$\hat{c}_i =
\begin{cases}
0, & y_i > 0, \\
1, & y_i \leq 0.
\end{cases}$$

По сути это наиболее вероятные значения отдельных символов кодового слова. По скольку 0 у нас передается как +1, а 1 как -1 жесткие, наиболее вероятные значения мы можем задать вот такой системой неравнеств.

Теперь сделаем следующее: вычтем, из каждого члена корреляционной функции значение $y_i$ домноженное на жесткое решение:

$$\arg\max_{c \in C} \sum_{i=0}^{n-1} (-1)^{c_i} y_i
= \arg\max_{c \in C} \sum_{i=0}^{n-1} ((-1)^{c_i} y_i - (-1)^{\hat{c}_i} y_i)
= \arg\max_{c \in C} \sum_{i : c_i \neq \hat{c}_i} -|y_i|
= \arg\min_{c \in C} E(c, y)$$

Поскольку жесткие решения не зависят от кодовых слов на результат максимизации это никак не влияет.

В итоге у нас получиться, что для тех $i$, где $c_i$ совпадают с жесткими решениями такая штука будет равна 0. Там где они не совпадают там получиться минус удвоенное значенияе $|y_i|$. 2-ку отбросим, в реузльтате у нас останется полученная нами вещь, что в свою очередь эквивалентно минимизации корреляционной невызки.

**Корреляционная невязка** $E(c, y)$ = $\sum_{i : c_i \neq \hat{c_i}}|y_i|$

Такой поход применим не только к каналу с АБГШ. Это можно сделать и для других, более сложных каналов, если в $y_i$ использовать логарифмические отношения правдоподобия:

$$L_i = \log \frac{P\{c_i = 0 \mid y_i\}}{P\{c_i = 1 \mid y_i\}}$$

## Метод порядковых статистик

Рассмотрим передачу кодовых слов ($c_0, ...., c_{n-1}$) двоичного ($n, k$) кодаа с помощью символов 2-АМ по каналу без памяти. Пусть $(y_0, ...., y_{n-1})$ - соотвествующие принятые символы.

Пример: $y_i = (-1)^{c_i} + \eta_i,  \eta_i \sim N(0, \sigma^2)$

Пусть $L_i = \log \frac{P\{c_i = 0 \mid y_i\}}{P\{c_i = 1 \mid y_i\}}$ - логарифмические отношения правдоподобия.

Пусть $\hat{c}_i =
\begin{cases}
0, & L_i > 0, \\
1, & L_i \leq 0.
\end{cases}$ - жесткие решения

Чем больше $y$, тем больше логарифмические отношения правдоподобия. Это означаает, что вероятность 0 оказывается больше вероятности 1. То есть чем больше по абсолютной величине y, тем больше отличаются эти вероятности. Это означает, что вероятность того, что мы ошибемся, принимая наши жесткие решения в данной задаче она убывает с увеличением абсолютной величины логарифмического отношения правдоподобия.

То есть вероятность ошибки в $\hat{c_i}$ убывает с увеличением $|L_i|$

Возникает естественная идея: давайте выберем информационную совокупность J кода, соотвествующую наибольшим значением $|L_i|$. После этого приведем порождающую матрицу кода к виду $G_J$ с единичной подматрицей в столбцах $J$

С большой вероятностью число неверных жестких решений $\hat{c_i}, i \in J$, мыло. Переберм все конфигурации ошибок $e$ веса не более $t$ на $J$ и построим кодовые слова $c_e = (\hat(c_J) + e)G_J$ Выберем наиболее правдоподобное из полученных кодовых слов.

Сложность такого алгоритма будет составлять $O(k^2n + \sum^t_{i=0}inC^i_k)$. $C^i_k$ - биномиальный коеффицент из k по i.

При $t = d/4$ достигается вероятность ошибки, близкая к вероятности ошибки декодирования по максимуму правдоподобия.

Можно ли усовершенствовать этот алгоритм?

Можно организовать перебор ошибок в порядке возрастания из корреляционной невязки. С большой вероятностью мы найдем правильную конфигурацию ошибок уже на начальных итерациях. И, если у нас есть какое-то кодовое слово $c$ с какой-то кореляционной невязкой E, а потом вдруг следующая по порядку конфигурация ошибок, которая уже сама по себе имеет большую конфигурационную невязку, то понятно, что она не может улучшить наше текущее состояние, поэтому ее можно сразу отбросить.

Можно также произвести размен времени на память. Можно в начале перебрать конфигурации ошибок веса не более чем $t$ и запомнить те символы кодового слова, которые получаются на каких-то позициях за пределами этой информационной совокупности, затем посмотреть какие из рассмотренных конфигураций ошибок давали одинаковые значения на вот этих вот вспомогательных позициях. Если мы сложим соотвествующие кодовые слова, то мы получим фактически конфигурацию ошибок, которая имеет удвоенный вес на наиболее надежной информационной совокупности. Приэтом при надлежащем выборе параметров число таких комбинаций будет небольшим. Здесь можно забесплатно удвоить рассматриваемое значени, что очень сильно помогает.
