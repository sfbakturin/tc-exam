# Алгоритм Питерсона-Горенстайна-Цирлера декодирования кодов БЧХ

Начнём с непосредственного введения в **декодирование кодов БЧХ**:

1. Рассмотрим случай передачи данных по дискретному симметричному каналу, такой, что его выход это $y = c + e$, где $c$ - кодовое слово, $e$ - вектор ошибок.
2. Передаваемый $y$ можно выразить в виде многочлена $y(x) = a(x)g(x) + e(x)$, где $a(x)$ - некий входной многочлен, $g(x)$ - порождающий многочлен, $e(x)$ - многочлен ошибок.
3. Давайте подставим в это равенство значения $x$, равные корням порождающего многочлена $g(x)$, то есть $\alpha^{b + i}$, для всех $i$. Получается такое:

    $$
      \begin{aligned}
        y\left(\alpha^{b + i}\right) &= \underbrace{a\left(\alpha^{b + i}\right)\cdot g\left(\alpha^{b + i}\right)}_{\text{поскольку}\ \alpha^{b + i}\ \text{это корень порождающего многочлена, то эта штука зануляется}} + e\left(\alpha^{b + i}\right) \\
        &= e\left(\alpha^{b + i}\right), ~ 0 \leqslant i < \delta - 1
      \end{aligned}
    $$

4. Предположим, что ошибки произошли в позициях $j_{1}, \ldots, j_{t}$, где $t \leqslant \left\lfloor{\dfrac{\delta - 1}{2}}\right\rfloor$ (пока что предполагаем, что количество ошибок меньше половины конструктивного расстояния, если будет больше, то не факт, что мы сможем кодировать однозначно). Компоненты вектора синдрома будут равны следующему:

    $$
      S_i = \sum_{r = 0}^{n - 1}{\left(e_r\alpha^{(b + i)r}\right)} = \sum_{l = 1}^{t}{\left(e_{j_l}\alpha^{(b + i)j_{l}}\right)}
    $$

5. Введём следующие обозначения: $E_l = e_{j_l}$ - значения ошибок, $X_l = \alpha^{j_l}$ - *локаторы* ошибок:

    $$
      S_i = \sum_{l = 1}^{t}{\left(E_lX_l^{b + i}\right)}, ~ 0 \leqslant i < \delta - 1
    $$

    Нам неизвестно ничего, кроме $S_i$.

6. *Поиск локаторов ошибок*.
    1. Введем ещё один многочлен, корнями которого являются величины, обратные к локаторам ошибок:

        $$
        \Lambda(x) = \prod_{l = 1}^{t}{\left(1 - X_lx\right)} = \sum_{l = 0}^{t}{\left(\Lambda_l x^l\right)}
        $$

    2. Корнями этого многочлена являются величины, обратные к локаторам ошибок:

        $$
        0 = \Lambda\left(X_i^{-1}\right) = \sum_{l = 0}^{t}{\left(\Lambda_l X_i^{-1}\right)}, ~ 1 \leqslant i < t
        $$

    3. Попробуем преобразовать всё это. Домножим на $E_iX_i^{b +j + t}$ и преобразуем:

        $$
        \begin{aligned}
        0 &= E_iX_i^{b+j+t} \sum_{l = 0}^{t}{\left(\Lambda_l X_i^{-1}\right)} \\
        &= \sum_{l = 0}^{t}{\left(\Lambda_l E_i X_i^{b + j + t}\right)} \\
        &= E_i X_i^{b + j + t} + \Lambda_1 E_i X_i^{b + j + t - 1} + \ldots + \Lambda_t E_i X_i^{b + j}, ~ 0 \leqslant j < t
        \end{aligned}
        $$

    4. Так как это выражение верно для всех $i$, то представим все-все элементы в виде сумм:

        $$
        \begin{aligned}
        0 &= E_i X_i^{b + j + t} + \Lambda_1 E_i X_i^{b + j + t - 1} + \ldots + \Lambda_t E_i X_i^{b + j} \\
        &= \underbrace{\sum_{i = 1}^{t}{\left(E_i X_i^{b + j + t}\right)}}_{S_{j + t}} + \Lambda_1 \underbrace{\sum_{i = 1}^{t}{\left(E_i X_i^{b + j + t - 1}\right)}}_{S_{j + t - 1}} + \ldots + \Lambda_t \underbrace{\sum_{i = 1}^{t}{\left(E_i X_i^{b + j}\right)}}_{S_j} \\
        &= S_{j + t} + \Lambda_1 S_{j + t - 1} + \ldots + \Lambda_t S_j
        \end{aligned}
        $$

    5. По итогу мы получили систему линейных уравнений следующего вида, который можно теперь решить:

        $$
        0 = S_{j + t} + \Lambda_1 S_{j + t - 1} + \ldots + \Lambda_t S_j
        $$

    6. Теперь мы можем представить всё это в виде матрицы:

        $$
        \Lambda_1 S_{j + t - 1} + \ldots + \Lambda_t S_j = -S_{j + t}
        $$

        $$
        \underbrace{
        \begin{pmatrix}
        S_0 & S_1 & \ldots & S_{t - 1} \\
        S_1 & S_2 & \ldots & S_{t} \\
        \vdots & \vdots & \ddots & \vdots \\
        S_{t - 1} & S_{t} & \ldots & S_{2t - 1}
        \end{pmatrix}
        }_{\mathbb{S}_t} \cdot
        \begin{pmatrix}
        \Lambda_t \\
        \Lambda_{t - 1} \\
        \vdots \\
        \Lambda_1
        \end{pmatrix} =
        \begin{pmatrix}
        -S_t \\
        -S_{t + 1} \\
        \vdots \\
        -S_{2t + 1}
        \end{pmatrix}
        $$

**Теорема**. $\mathbb{S}_t$ обратима, если $z$ равно числу произошедших ошибок $t$, и вырождена, если $z > t$.

Вооружившись этой теоремой, мы можем наконец сформулировать декодирование БЧХ или **алгоритм Питерсона-Горенстайна-Цирлера**:

1. Посчитаем синдром $S_i = y\left(\alpha^{b + i}\right)$, где $0 \leqslant i < \delta - 1$. Это можно сделать с помощью так называемой *схемы Горнера* - его асимпотитика - $\mathcal{O}(n\cdot (\delta - 1))$.
2. Далее будем подбирать число ошибок, начиная с наибольшего $\left\lfloor\dfrac{\delta - 1}{2}\right\rfloor$ двигаясь вниз, до сих пор, пока матрица $\mathbb{S}_t$ не станет обратимой. Проверку на обратимость можно сделать с помощью того же Гаусса с общей сложностью $\mathcal{O}(t^3)$.
3. Дальше с помощью Гаусса решить СЛАУ и *найти коэффициенты* $\Lambda_i$, где $1 \leqslant i \leqslant t$, многочлена локаторов ошибок $\Lambda(x) = 1 + \displaystyle\sum_{i = 1}^{t}{\Lambda_i x^i}$.
4. Затем можно *найти корни у многочлена локатора ошибок* с теми самыми коэффициентами $\Lambda_i$: $X_i = \alpha^{j_i}\ :\ \Lambda(X_i^{-1}) = 0$, $1 \leqslant i \leqslant t$. Здесь эффективнее всего воспользоваться перебором: перебираем все элементы конечного поля, подставляем в многочлен и смотрим получится ли ноль или не получится. Это и выше решение СЛАУ даёт $\mathcal{O}\left(\left\lfloor\dfrac{\delta - 1}{2}\right\rfloor^{4}\right)$.
5. Итак, мы нашли *локаторы ошибок*, то есть $X_l$, теперь остаётся *найти значения ошибок* $E_l$ - $S_i = \sum_{l = 1}^{t}{\left(E_l X_l^i\right)}$, $0 \leqslant i < t$ - теперь это система линейных уравнений, можем решить с помощью метода Гаусса и найти значения ошибок. асимпотитика: $\mathcal{O}(t^3)$.

Получили полиномиальную сложность относительно длины кода и его конструктивного расстояния.
